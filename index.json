[{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/","title":"Worklog","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOn this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting Started with AWS, IAM \u0026amp; Billing\nWeek 2: Networking: VPC Architecture, Subnets, Route Tables \u0026amp; Security Groups\nWeek 3: Compute \u0026amp; Scaling: EC2, EBS, Load Balancing \u0026amp; Auto Scaling\nWeek 4: Storage \u0026amp; CDN: S3, Static Website Hosting \u0026amp; CloudFront\nWeek 5: Database: RDS (SQL), DynamoDB (NoSQL) \u0026amp; Multi-AZ\nWeek 6: General Review \u0026amp; Integrated Lab Practice (VPC + EC2 + RDS + S3)\nWeek 7: Serverless: Lambda Functions \u0026amp; API Gateway\nWeek 8: Containers: Docker, ECR \u0026amp; Amazon ECS Fargate\nWeek 9: Infrastructure as Code (IaC) with AWS CDK\nWeek 10: DevOps: Building CI/CD Pipelines (CodeBuild, CodePipeline)\nWeek 11: System Monitoring \u0026amp; Logging with CloudWatch \u0026amp; CloudTrail\nWeek 12: Final Review: Integrating Serverless, Containers \u0026amp; DevOps, Workshop 1\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/5-workshop/5.3-s3/create-s3/","title":"Create S3","tags":[],"description":"","content":"Create S3 Open the S3\nClick Create Bucket:\nIn the Create Bucket console: Specify name of the bucket: s3-demo-text\nDo not add a tag to the VPC endpoint at this time. Click Create bucket Then click on the bucket you just created and press Create folder In the Create folder console: Specify name of the bucket: input then click Create folder We do the same with creating the output file. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"INTRODUCTION Introduction to S3 Event Notifications S3 Event Notifications is a feature that allows Amazon S3 to automatically send notifications when specific events occur in your Bucket (for example: when a new file is uploaded, deleted, or copied). S3 can send notifications to various destinations such as AWS Lambda, Amazon SNS, or Amazon SQS. In this workshop, we use the PutObject event to automatically trigger a Lambda function as soon as a text file is uploaded to S3. Introduction to Amazon Polly Amazon Polly is a Text-to-Speech service that uses AWS\u0026rsquo;s advanced Deep Learning technology. Polly supports over 60 voices in more than 20 languages, including Vietnamese, with natural human-like audio quality. This service is fully managed, so you don\u0026rsquo;t need to worry about managing infrastructure or scalability. Workshop Overview In this workshop, you will build a Serverless Text-to-Speech Converter application. The system operates fully automatically based on an Event-driven Architecture model:\nAmazon S3 (Input Bucket): Stores input text files (.txt) uploaded by users. S3 Event Notifications: Detects new file upload events and triggers AWS Lambda. AWS Lambda: The central processor that orchestrates data flow between S3 and Polly. Lambda reads the text file, calls the Polly API for conversion, and saves the result. Amazon Polly: The AI service that converts text into audio with natural voice. Amazon S3 (Output Bucket): Stores output audio files (.mp3) after processing is complete. The entire process runs automatically without manual intervention, helping save time and operational costs.\nSystem Architecture The model below describes the detailed architecture and data flow of the Text-to-Speech Converter system:\nWorkflow:\nUpload file: User uploads a text file (.txt) to Amazon S3 Input Bucket.\nEvent trigger: S3 detects the PutObject event (new file), automatically sends an event notification to trigger AWS Lambda Function.\nLambda processing: Lambda function is triggered and performs:\nReads the text file content from S3 Input Bucket Sends the text content to Amazon Polly API along with parameters (voice ID, output format) Polly conversion: Amazon Polly receives the request, processes the text, and returns an audio stream to Lambda.\nSave result: Lambda receives the audio stream from Polly and saves it as an .mp3 file to Amazon S3 Output Bucket.\nComplete: User can download the MP3 file from S3 Output Bucket for use.\nBenefits of Serverless Architecture No server management: AWS automatically handles scaling, patching, and high availability. Cost optimization: Pay only when processing requests (pay-per-use model). Auto-scaling: The system can handle from a few requests to millions of requests without additional configuration. Fast deployment: Focus on application logic instead of managing infrastructure. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/4-eventparticipated/4.4-event4/","title":"Event AWS Cloud Mastery Series #3","tags":[],"description":"","content":"Summary Report: “Identity \u0026amp; Access Management” Event Objectives Understand IAM concepts, credential management, and AWS Organization structure. Master the threat detection system with GuardDuty and the Detection-as-Code mindset. Design a Layered Security network architecture and implement an optimized Network Firewall. Gain deep understanding of data encryption mechanisms with AWS KMS (Key Management Service): The difference between AWS Managed Keys and Customer Managed Keys (CMK). Apply automated traffic filtering strategies (Automated Domain Lists) to block emerging threats. Speakers FCJ members Key Highlights Identity \u0026amp; Access Management (IAM) Principles: Eliminate long-term Access Keys; use short-term credentials via IAM Identity Center. Controls: Combine SCPs (blocking permissions at the organization level) and Permission Boundaries (limiting permissions at the user level). Automation: Automatically rotate DB passwords with Secrets Manager. Detection \u0026amp; Response GuardDuty: Real-time monitoring (Runtime Monitoring) to detect anomalous behavior deep within the OS (processes, file access). Detection-as-Code: Manage detection rules as code (CloudFormation/Terraform) to ensure consistency and compliance. Network Security Layered Security: Combine WAF (Layer 7) -\u0026gt; Network Firewall (Layer 3-7) -\u0026gt; NACL (Subnet) -\u0026gt; Security Group (Instance). Network Firewall: Use Active Threat Defense to automatically update blocking rules against the latest threats from AWS Threat Intelligence. Automated Domain Lists feature: Automatically analyze HTTP/HTTPS traffic to create blocking or allowing rules based on domain names (FQDN) without manual IP management. Data Protection KMS Mechanism: KMS manages the Master Key (never leaves KMS). The Master Key is used to encrypt the Data Key. The Data Key (plaintext) is what actually encrypts user data. Key Classification: AWS Managed Key: Free, managed by AWS; users cannot manually rotate or change policies. Customer Managed Key (CMK): Paid; users have full control (rotation, permissions, deletion); mandatory for high compliance requirements. EBS Encryption: The process of encrypting EBS volumes involves creating a Data Key, encrypting the volume with that Data Key, and storing the encrypted Data Key with the volume. Key Takeaways Data Protection Strategy Use CMK for sensitive data: Although AWS Managed Keys are convenient, CMKs allow tighter control over who can use the key for decryption (via Key Policy). Key Rotation: Enable automatic key rotation every year (for CMK) to ensure safety if an old key is compromised. Technical Architecture Optimize Network Costs: Use the Multiple VPC Endpoints model for Network Firewall to reduce operational costs and simplify network architecture. Security Group Reference: Instead of whitelisting hardcoded IPs in Security Groups, reference the SG IDs of other tiers (e.g., SG-App only allows traffic from SG-Web) for greater flexibility. Application to Projects Improve Identity Management (IAM) Audit \u0026amp; Clean-up: Review all IAM Users in the project; delete old/unused Access Keys. Mandate MFA activation (preferably FIDO2/YubiKey) for all accounts. Implement Secrets Manager: Replace environment variables containing DB passwords in code (e.g., Spring Boot/Node.js) with code that calls the API to retrieve secrets from AWS Secrets Manager. Enhance Network Security Egress Filtering: Deploy AWS Network Firewall or DNS Firewall to control traffic from servers to the Internet, blocking connections to strange domains or C2 servers. Review Security Groups: Convert rules currently using static IPs to use Security Group Referencing to better support Auto Scaling. Monitoring \u0026amp; Automated Response Activate GuardDuty: Enable GuardDuty in the Production environment to immediately detect anomalous behaviors (like crypto mining, port scanning). Automated Remediation: Write a simple Lambda function connected to EventBridge: When GuardDuty reports a \u0026ldquo;High\u0026rdquo; severity issue, automatically revoke session tokens or block the Security Group of that instance. Event Experience Participating in \u0026ldquo;AWS Mastery #3\u0026rdquo; helped me systematize all security layers on AWS. As a developer who usually focuses on code rather than infrastructure, this event truly changed my mindset regarding security responsibilities:\nChanging the mindset on \u0026ldquo;Clean Code\u0026rdquo; Previously, I thought not hardcoding passwords was enough. But seeing the IAM Access Analyzer demo, I realized that writing Infrastructure-as-Code (Terraform/CloudFormation) also needs to be \u0026ldquo;linted\u0026rdquo; (checked for errors) for security logic. A single accidental Principal: * line in the code can throw the doors open for hackers, and this tool acts like a \u0026ldquo;Unit Test\u0026rdquo; for policies. Mélofee Malware Analysis Seeing the wget http://173.209\u0026hellip; command connecting directly to an IP instead of a domain was a \u0026ldquo;wake-up call.\u0026rdquo; I used to think just blocking DNS was safe, but real malware is much smarter. This proves why Network Firewall (blocking IP egress) is so critical for backend servers. The \u0026ldquo;Cost\u0026rdquo; Lesson A small but expensive detail the speaker shared: The Free Tier account immediately expires upon joining an AWS Organization. This is extremely useful information for Devs who use personal accounts for tinkering, helping to avoid \u0026ldquo;out of the blue\u0026rdquo; bills. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/4-eventparticipated/4.3-event3/","title":"Event AWS Cloud Mastery Series #2","tags":[],"description":"","content":"Summary Report: “DevOps on AWS” Event Objectives Analyze the shift from manual operations (ClickOps) to Infrastructure as Code (IaC) to eliminate human error and inconsistency. Master the operational mechanisms of AWS CloudFormation in defining and managing the lifecycle of cloud resources. Master AWS CDK (Cloud Development Kit) to build infrastructure using high-level programming languages through Construct levels. Gain deep understanding of Docker technology, the application packaging process, and image management with Amazon ECR. Compare and select the appropriate Container orchestration solution among Amazon ECS, Amazon EKS, and AWS App Runner. Speakers FCJ members Key Highlights Infrastructure Mindset \u0026amp; Configuration Management Limitations of ClickOps: Operations via the Management Console pose high risks of human error, slow speed, and collaboration difficulties. CloudFormation Stack: Manages a collection of resources as a single unit, allowing for the synchronized creation, update, or deletion of the entire infrastructure. Drift Detection: A crucial mechanism to identify when actual resource configurations have been manually changed and no longer match the original definition template. Infrastructure Programming with AWS CDK Multi-language: Supports infrastructure definition using TypeScript, JavaScript, Python, Java, C#/.NET, and Go. Constructs System: L1 Constructs: 1:1 mapping with CloudFormation resources (Cfn), requiring detailed configuration. L2 Constructs: Provide optimized defaults and helper methods. L3 Constructs (Patterns): Complete architectural patterns, combining multiple resources to solve a specific problem. Workflow: From initialization (cdk init), template synthesis (cdk synth), to deployment (cdk deploy) and destruction (cdk destroy). Container Ecosystem \u0026amp; Orchestration Docker Fundamentals: Distinguish between Containers (lightweight, shared kernel) and Virtual Machines (heavy, includes Guest OS) and the Build - Push - Pull - Run process. Amazon ECR: Secure container image repository, supporting vulnerability scanning and lifecycle policies. Orchestration Models: Amazon ECS: AWS-native solution, simple, deeply integrated, supports running on EC2 or Fargate (Serverless). Amazon EKS: Managed Kubernetes service, providing flexibility and open-source standardization but requiring more complex operations. AWS App Runner: PaaS solution enabling rapid web application deployment directly from source code or images without infrastructure management. Key Takeaways IaC Strategy Code over Clicks: Completely shift to the \u0026ldquo;No More ClickOps\u0026rdquo; model to ensure consistency and scalability. Version Control: Use templates (YAML/JSON) or CDK source code as the single blueprint for infrastructure. Change Management: Frequently use cdk diff and Drift Detection to control differences between source code and reality before deployment. Technical Architecture Immutability: Docker Images and Containers ensure consistent application execution across all environments (\u0026ldquo;It works on my machine\u0026rdquo; -\u0026gt; \u0026ldquo;Ship your machine\u0026rdquo;). Compute Selection: Use AWS Fargate to remove server management burdens (Serverless), or EC2 when deep control and cost optimization for long-running tasks are needed. Abstraction Levels: Leverage L2/L3 Constructs in CDK to reduce code volume and inherit built-in best practices. Application to Projects Deploy App Runner: Use for web applications or APIs requiring rapid deployment (prototypes) without wanting to manage servers. Optimize ECS: Apply the ECS model combined with Application Load Balancer (ALB) for standard microservices architectures as seen in the \u0026ldquo;Cats vs Dogs\u0026rdquo; demo. Switch to CDK: Start writing infrastructure in TypeScript/Python instead of raw CloudFormation to accelerate development and code reusability. Event Experience Participating in the “AWS Mastery #2” session provided deep insights into modernizing operations through infrastructure coding and containers. Practical experiences included:\nLearning from Experts Understand the core differences between Terraform (multi-platform, uses HCL) and AWS CDK/CloudFormation (optimized for AWS, uses programming languages) to select the appropriate tool. Grasp the Construct Tree structure of a CDK application, from App to Stack and Resources. Lessons Learned Tool Selection: ECS or App Runner are the best choices for teams wanting lower ops overhead, while EKS is for complex requirements and high portability. Automation is Mandatory: Integrating CI/CD with CodePipeline and CodeBuild is key to maintaining stability when deploying distributed container architectures. Standardization: Using Dockerfile and ECR helps standardize the runtime environment, completely eliminating errors caused by environmental differences. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/4-eventparticipated/4.2-event2/","title":"Event AWS Cloud Mastery Series #1","tags":[],"description":"","content":"Summary Report: “AWS AI/ML Services \u0026amp; Generative AI Workshop” Event Objectives Provide an overview of the AI/ML landscape in Vietnam. Demonstrate end-to-end machine learning workflows using Amazon SageMaker. Introduce Generative AI capabilities with Amazon Bedrock (Foundation Model, Guardrails,\u0026hellip;). Share techniques for Prompt Engineering and Retrieval-Augmented Generation (RAG). Speakers FCJ members in AWS\nKey Highlights AWS AI/ML Services Overview (SageMaker) End-to-End Platform: Covered the full lifecycle from data preparation to deployment. Data Preparation: Strategies for labeling and cleaning data. Training \u0026amp; Tuning: optimizing models for performance and cost. Deployment: Moving models to production endpoints efficiently. Integrated MLOps: Highlighted capabilities to automate and standardize ML pipelines. SageMaker Studio: Live demo of the unified interface for building, training, and deploying models. Generative AI with Amazon Bedrock Foundation Models (FMs): Comparison and selection guide for top models: Claude: High reasoning capabilities. Llama: Open and efficient. Titan: Native AWS integration. Prompt Engineering: Chain-of-Thought: Breaking down complex reasoning tasks. Few-shot learning: Using examples to improve model output. Advanced Architectures: RAG (Retrieval-Augmented Generation): Integrating Knowledge Bases to ground answers in company data. Bedrock Agents: Creating multi-step workflows and integrating with external tools. Guardrails: Ensuring safety and content filtering. Key Takeaways Design Mindset Model Selection: Choose the right Foundation Model (e.g., Claude vs. Titan) based on the specific use case requirements (speed vs. reasoning). Safety First: Implementation of Guardrails is critical for responsible AI deployment in enterprise settings. Technical Architecture RAG over Fine-tuning: For most internal knowledge use cases, RAG provides a more flexible and cost-effective solution than fine-tuning models. Agentic Workflows: Moving from passive chatbots to active Agents that can execute tasks is the next frontier of GenAI. Modernization Strategy MLOps Adoption: Moving away from manual model training to automated pipelines (MLOps) is essential for scalability. Local Context: Understanding the specific AI/ML trends and landscape within Vietnam helps in benchmarking local projects. Applying to Work Pilot SageMaker: Evaluate current ML workflows and identify opportunities to migrate to Amazon SageMaker for better lifecycle management. Build a Knowledge Bot: Create a prototype using Amazon Bedrock and RAG to query internal documentation or technical manuals. Refine Prompting: Immediately apply Chain-of-Thought and Few-shot techniques to improve the accuracy of current AI interactions. Implement Guardrails: Configure content filters on Bedrock to ensure brand safety for any experimental applications. Event Experience Attending the “AWS AI/ML Services \u0026amp; Generative AI Workshop” provided a practical roadmap for adopting intelligent services. Key experiences included:\nLearning from experts Gained clarity on the AI/ML landscape in Vietnam, understanding local opportunities and challenges. Deepened technical knowledge on the differences between major Foundation Models (Claude, Llama, Titan). Hands-on technical exposure The SageMaker Studio walkthrough demonstrated how to unify the fragmented ML toolchain into a single pane of glass. The Live Demo of Bedrock showed the practical implementation of building a Generative AI chatbot, demystifying the complexity of RAG and Agents. Networking and discussions The ice-breaker activity and networking sessions allowed for exchanging ideas with peers about real-world challenges in deploying GenAI. Discussions reinforced the importance of Prompt Engineering as a critical skill for modern development. Lessons learned RAG is the key to making LLMs useful for business-specific data without the high cost of training. Guardrails are not optional; they are a fundamental layer of the Generative AI stack. Efficiency in ML comes from integrated MLOps rather than isolated data science experiments. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Shift mindset from \u0026ldquo;ClickOps\u0026rdquo; (manual clicking) to \u0026ldquo;IaC\u0026rdquo; (Infrastructure as Code). Use AWS CDK to define infrastructure. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn IaC (Infrastructure as Code) concepts - Introduction to CloudFormation and AWS CDK (Cloud Development Kit) 03/11/2025 03/11/2025 3 - Practice: Install AWS CDK, initialize a CDK project (TypeScript/Python) - Understand App, Stack, and Construct structure 04/11/2025 04/11/2025 4 - Practice: Write CDK code to create an S3 Bucket and DynamoDB Table - Learn cdk synth and cdk deploy commands 05/11/2025 05/11/2025 5 - Advanced Practice: Write CDK code to rebuild the VPC (similar to Week 2) - Manage dependencies between resources in code 06/11/2025 06/11/2025 6 - Execute cdk destroy to clean up resources. Understand Drift detection. 07/11/2025 07/11/2025 Week 9 Achievements: Understood the benefits of IaC in version control and infrastructure reusability.\nCapable of using programming languages to automate AWS resource provisioning instead of manual creation.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Understand Containerization on AWS. Deploy Docker applications on AWS ECS Fargate. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review Docker: Dockerfile, Image, Container - Learn about Amazon ECR (Elastic Container Registry) 27/10/2025 27/10/2025 3 - Practice: Build a Docker Image for a simple NodeJS app - Push the Image to an ECR repository 28/10/2025 28/10/2025 4 - Learn about Amazon ECS (Elastic Container Service) - Distinguish between EC2 Launch Type and Fargate (Serverless compute for containers) 29/10/2025 29/10/2025 5 - Practice: Create an ECS Cluster, Task Definition, and Service - Deploy containers running on the Fargate platform 30/10/2025 30/10/2025 6 - Verify running applications, view container logs on AWS. 31/10/2025 31/10/2025 Week 8 Achievements: Learned how to package applications and manage Docker Images on ECR.\nSuccessfully deployed containerized applications to a production-ready environment using ECS Fargate without managing physical servers.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Familiarize with Serverless concepts. Build a fully serverless REST API. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn AWS Lambda: Runtimes, Handlers, Triggers - Lambda limits (Timeout, Memory) 20/10/2025 20/10/2025 3 - Practice: Write a simple Lambda Function (Hello World) using Python/NodeJS - Test Lambda function with test events 21/10/2025 21/10/2025 4 - Study Amazon API Gateway: Resources, Methods, Stages - Integrate API Gateway with Lambda 22/10/2025 22/10/2025 5 - Practice: Build a simple CRUD API (Create, Read, Update, Delete) - Save data from Lambda to DynamoDB 23/10/2025 23/10/2025 6 - Test API using Postman/Curl. Monitor logs via CloudWatch. 24/10/2025 24/10/2025 Week 7 Achievements: Understood how to run code without managing server infrastructure.\nSuccessfully built a Serverless API Backend using API Gateway, Lambda, and DynamoDB.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: General Review: Consolidate and systematize knowledge from Week 1 to Week 5. Integrated Practice: Self-deploy a combined model (VPC + EC2 + RDS + S3) with minimal reliance on documentation. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review Week 1 \u0026amp; 2: IAM, Billing, and VPC (Critical). - Redraw the deployed VPC network diagram to verify understanding of Routing/Subnets. 13/10/2025 13/10/2025 3 - Review Week 3: EC2, Load Balancer, Auto Scaling. - Review debugging steps when unable to SSH into servers. 14/10/2025 14/10/2025 4 - Review Week 4 \u0026amp; 5: S3, CloudFront, RDS, DynamoDB. - Compare pros/cons of SQL (RDS) and NoSQL (DynamoDB). 15/10/2025 15/10/2025 5 - Integrated Lab Practice: Re-deploy a 2-tier model (Web Server connecting to Database) in a Custom VPC. - Configure S3 to store backup files from EC2. 16/10/2025 17/10/2025 6 - Resource Cleanup: Remove all old/unused labs to optimize costs. - Compile common errors (Troubleshooting log) into a personal notebook. 17/10/2025 17/10/2025 Week 6 Achievements: Systematized the overall picture of basic AWS infrastructure (Compute, Network, Storage, Database).\nGained confidence in deploying and debugging common connectivity errors within a VPC environment.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives: Distinguish between and utilize RDS (SQL) and DynamoDB (NoSQL). Understand Multi-AZ architecture for Databases. Practice: Connect applications to the Database. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Amazon RDS, supported engines (MySQL, Postgres\u0026hellip;) - Concepts of Multi-AZ and Read Replicas 06/10/2025 06/10/2025 3 - Practice: Create an RDS Instance (MySQL) in a Private Subnet - Configure Security Group to allow EC2 connection to RDS 07/10/2025 07/10/2025 4 - Study Amazon DynamoDB (NoSQL): Table, Item, Partition Key, Sort Key - Capacity modes (Provisioned vs. On-demand) 08/10/2025 08/10/2025 5 - Practice: Create a DynamoDB table, add/edit/delete items via AWS Console and AWS CLI - Basic understanding of ElastiCache (Redis) 09/10/2025 09/10/2025 6 - Practice: Write a small script (Python/NodeJS) on EC2 to query data from RDS and DynamoDB. 10/10/2025 10/10/2025 Week 5 Achievements: Successfully created and configured a secure RDS MySQL instance in a Private Subnet.\nUnderstand basic table design with DynamoDB.\nSuccessfully established a connection from the Web Server to the Database Server.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Learn Amazon S3 object storage service.* Understand basic AWS services, how to use the console \u0026amp; CLI. Accelerate static content access using CloudFront (CDN). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Amazon S3: Buckets, Objects, Storage Classes - Learn about S3 Security (Bucket Policy, ACLs) 29/09/2025 29/09/2025 3 - Practice: Create an S3 Bucket, upload files, enable Versioning - Configure S3 Static Website Hosting 30/09/2025 30/09/2025 4 - Learn Amazon CloudFront (CDN), Edge Locations - Understand Origin and Behavior concepts in CloudFront 01/10/2025 01/10/2025 5 - Practice: Create a CloudFront Distribution with S3 Bucket as the Origin - Configure OAC (Origin Access Control) to secure S3 02/10/2025 02/10/2025 6 - Review S3 and CloudFront 03/10/2025 03/10/2025 Week 4 Achievements: Gained a clear understanding of S3 storage classes for cost optimization.\nSuccessfully hosted a static website (HTML/CSS/JS) on S3.\nIntegrated CloudFront to enable faster website loading and enhanced security (HTTPS).\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 3 Objectives: Understand EC2 virtual server services and EBS volume types. Understand High Availability and Scalability concepts. Practice: Deploy a web server with Load Balancer and Auto Scaling. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn EC2 Instance types, AMIs, and Key Pairs - Learn about EBS Volumes and Snapshots 22/09/2025 22/09/2025 3 - Practice: Launch a Linux EC2 instance, SSH into the server, install basic Apache/Nginx - Attach an additional EBS Volume to the EC2 23/09/2025 23/09/2025 4 - Study Elastic Load Balancer (ALB vs. NLB) - Learn about Auto Scaling Groups (ASG) and Launch Templates 24/09/2025 24/09/2025 5 - Practice: Create an Application Load Balancer pointing to EC2 - Configure Target Groups and Health Checks 25/09/2025 25/09/2025 6 - Lab Practice: Simulate high traffic to trigger Auto Scaling to automatically launch new servers. 26/09/2025 26/09/2025 Week 3 Achievements: Successfully deployed a basic Web Server on EC2.\nUnderstood and configured Load Balancer for traffic distribution.\nEstablished an Auto Scaling Group to allow the system to automatically scale based on actual demand.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/4-eventparticipated/4.1-event1/","title":"Event Cloud Day","tags":[],"description":"","content":"Track 1 : Gen AI and Data Summary Report: “Building a Unified Data Foundation on AWS for AI and Analytics Workloads” Event Objectives Share strategies and best practices for constructing a unified, scalable data foundation Tailor data infrastructure to support AI and analytics workloads Leverage AWS services to create a robust environment for modern applications Cover key components including data ingestion, storage, processing, and governance Enable effective data management for advanced analytics and AI initiatives Speakers Kien Nguyen – Solutions Architect, AWS Key Highlights The Rise of Agentic AI Moving from Chatbot -\u0026gt; Agentic Systems (autonomous, multi-agent, mimicking human logic). 15% of day-to-day work decisions will be made autonomously by AI in 2028 Amazon Bedrock AgentCore Security: Deploy agents securely at scale with enterprise-grade controls. Capabilities: Enhance agents with Tools (APIs) to take action and Memory to retain customer context. Observability: Full visibility to monitor agent reasoning and actions. Key Takeaways Strategic Mindset Autonomy over Automation: The goal is to build systems that can independently achieve goals, not just automate repetitive tasks Risk Awareness: Innovation must go hand-in-hand with risk controls and clear business value definition. Technical Architecture Vector Search: Is the backbone of AI memory; essential for RAG workflows. Multi-Agent Systems: The future lies in specialized agents collaborating (e.g., Guardian Agent + FAQ Agent). Applying to Work Assess Data Readiness Evaluate current databases for Vector Search compatibility to support AI memory. Define Risk Controls: Establish governance policies for AI prompts and responses before scaling. Try Bedrock: Start experimenting with AgentCore to build a prototype with simple tool integration. Event Experience Attending the “Building a Unified Data Foundation on AWS for AI and Analytics Workloads” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from market insights Understood the urgency of adoption: Gartner predicts 33% of software will use Agentic AI by 2028. Learned why 40% of projects might fail (cost, risk controls) and how to avoid these pitfalls. Technical visualization Visualized the architecture of Agentic Systems: How agents perceive, reason, and act autonomously. Saw the direct connection between Data Strategy (Vector DBs) and Agent Capabilities (Memory). "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/","title":"Internship Report","tags":[],"description":"","content":"Internship Report ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Đỗ Quốc Khánh\nPhone Number: 0862174951\nEmail: doquockhanhcntt@gmail.com\nUniversity: FPT University HCMC\nMajor: Information Technology\nStudent Code: SE194843\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 2 Objectives: Understanding of AWS network architecture: VPC, Subnets, Route Tables. Understand network security mechanisms: Security Groups vs. NACLs. Practice building a Custom VPC instead of using the default one Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS Virtual Private Cloud, Regions and Availability Zones (AZ) - Understand CIDR Blocks and IP addressing/subnetting 15/09/2025 15/09/2025 3 - Learn about Subnets (Public/Private), Route Tables, Internet Gateway (IGW) - Learn about NAT Gateway (enabling internet access for Private subnets) 16/09/2025 16/09/2025 4 Practice: Create a Custom VPC, create 2 Subnets (1 Public, 1 Private), attach an Internet Gateway 17/09/2025 17/09/2025 5 - Distinguish between Security Groups (Stateful) and Network ACLs (Stateless) - Configure basic Inbound/Outbound rules (SSH, HTTP) 18/09/2025 18/09/2025 6 - Practice: Finalize the 2-tier VPC model. Test network connectivity between subnets and to the internet. 19/09/2025 19/09/2025 Week 2 Achievements: Clearly understood packet traffic flow within a VPC environment.\nSuccessfully built a secure VPC architecture including: a Public Subnet (for Web Server) and a Private Subnet (for future Database).\nLearned how to configure Security Groups to open only necessary ports (Principle of Least Privilege).\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services Practice create account AWS free tier Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Connect with FCJ members - Read and take note of internship unit rules and regulations 08/09/2025 08/09/2025 3 - Learn about AWS account - Practice: Create AWS Free Tier account 09/09/2025 09/09/2025 4 Introduction to cloud computing 10/09/2025 10/09/2025 5 - Learn AWS account and cost management tools, - Learn about AWS support 11/09/2025 11/09/2025 6 - Practice: Create buget for account and admin group and admin user 12/09/2025 12/09/2025 Week 1 Achievements: Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/5-workshop/5.4-lambda/create-lambda/","title":"Create Lambda","tags":[],"description":"","content":"Create Lambda function Navigate to Lambda management console In the Dashboard console, choose Create function In the Create Lambda function console Name the lambda: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Runtime: choose python 3.13 or latest Expand the Change default execution role section In Execution role, choose Use an existing role and choose PollyLambdaRole Scroll down and choose Create lambda Successfully create S3 bucket. Create code Lambda In this workshop, we will use the code to convert a text folder into a voice folder (Text-to-Speech) using Amazon Polly service. Scroll down to the Code source section. Delete all existing code in the lambda_function.py file. We will use the code as follows: import json import boto3 import os s3 = boto3.client(\u0026#39;s3\u0026#39;) polly = boto3.client(\u0026#39;polly\u0026#39;) def lambda_handler(event, context): try: # 1. Get uploaded file info record = event[\u0026#39;Records\u0026#39;][0] bucket_name = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] object_key = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] # Ex: input/hello.txt print(f\u0026#34;Processing file: {object_key}\u0026#34;) # 2. Read text file content file_obj = s3.get_object(Bucket=bucket_name, Key=object_key) text_content = file_obj[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) # 3. Call Polly to convert text to speech (Voice: Joanna) response = polly.synthesize_speech( Text=text_content, OutputFormat=\u0026#39;mp3\u0026#39;, VoiceId=\u0026#39;Joanna\u0026#39; ) # 4. Save MP3 file to output folder # Change name from input/abc.txt to output/abc.mp3 new_key = object_key.replace(\u0026#34;input/\u0026#34;, \u0026#34;output/\u0026#34;).replace(\u0026#34;.txt\u0026#34;, \u0026#34;.mp3\u0026#34;) if \u0026#34;AudioStream\u0026#34; in response: with response[\u0026#34;AudioStream\u0026#34;] as stream: s3.put_object( Bucket=bucket_name, Key=new_key, Body=stream.read(), ContentType=\u0026#39;audio/mpeg\u0026#39; ) return \u0026#34;Done!\u0026#34; except Exception as e: print(e) raise e Click the Deploy button. Create Trigger Right above the code section, click the + Add trigger button. Source: Select S3. Bucket: Select the bucket s3-demo-text. Event types: Select All object create events. Prefix: Enter input/ ⚠️ Note: You must enter input/. If left blank, Lambda will trigger even when the MP3 file is created -\u0026gt; Causing an infinite loop -\u0026gt; Increasing costs. Suffix: Enter .txt Check the box \u0026ldquo;I acknowledge\u0026hellip;\u0026rdquo; -\u0026gt; Click Add. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.Since I am performing this workshop for the first time, I will grant full access to the selected permissions.\n{ { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;polly:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;logs:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } } "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Systematize knowledge regarding Serverless (Lambda) and Containers (ECS). Reinforce skills in Infrastructure as Code (CDK) and CI/CD Pipelines. Practice: Workshop 1 - Automated Text-to-Speech Converter using Serverless Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review Weeks 7 \u0026amp; 8: Compare pros and cons of Serverless (Lambda) vs. Containers (ECS). - Review optimal Docker Image packaging techniques. 24/11/2025 26/11/2025 3 - Review Week 9 (IaC): Re-read old CDK code, ensuring understanding of Construct and Stack structures. - Try writing a short CDK snippet to create a new resource (e.g., SNS Topic) to test retention/reflexes. 27/11/2025 28/11/2025 4 - Review Week 10 (CI/CD): Redraw the Pipeline execution flow (Source -\u0026gt; Build -\u0026gt; Deploy). - Review common Build errors (missing permissions, incorrect config files). 29/11/2025 30/11/2025 5 - Review Week 11 (Monitoring): Check if yesterday\u0026rsquo;s Integrated Lab has Logs and Metrics available. 1/12/2025 1/12/2025 6 - Practice Workshop 1 Automated Text-to-Speech Converter using Serverless 03/12/2025 07/12/2025 Week 12 Achievements: Successfully linked the connections between DevOps tools (Code -\u0026gt; Build -\u0026gt; Deploy -\u0026gt; Monitor).\nGained confidence in debugging permission-related issues (IAM Roles) within Pipelines and Lambda.\nCompleted the challenging Integrated Lab (IaC + CI/CD + Containers)\nCompleted workshop 1 and demo\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Monitor system health and troubleshoot incidents. Audit activities on the AWS account. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Amazon CloudWatch: Metrics, Logs, Alarms - Standard EC2 metrics (CPU, Network\u0026hellip;) 17/11/2025 17/11/2025 3 - Practice: Create a CloudWatch Alarm to send email via SNS when EC2 CPU \u0026gt; 80% - Install CloudWatch Agent to collect RAM metrics (Memory usage) 18/11/2025 18/11/2025 4 - Learn AWS CloudTrail (Audit logs) - Learn AWS X-Ray (Tracing for microservices) 19/11/2025 19/11/2025 5 - Practice: Create a Dashboard on CloudWatch for a system overview - Check CloudTrail to see who deleted resources 20/11/2025 20/11/2025 6 - Review CouldTrail and CloudWatch 21/11/2025 21/11/2025 Week 11 Achievements: Capable of real-time system monitoring.\nEstablished an early incident warning system.\nLearned how to audit sensitive actions on the AWS account.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Automate the software development process (DevOps). Build a complete CI/CD pipeline on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Learn CI/CD (Continuous Integration / Continuous Delivery) - Introduction to the toolset: CodeCommit, CodeBuild, CodeDeploy, CodePipeline 10/11/2025 10/11/2025 3 - Practice: Create a Repository (can use GitHub), connect with CodeBuild - Write buildspec.yml file to build code 11/11/2025 11/11/2025 4 - Study CodePipeline: Source stage, Build stage, Deploy stage - Practice: Create a simple Pipeline to deploy files to S3 12/11/2025 12/11/2025 5 - Advanced Practice: Create a Pipeline to deploy applications to ECS or Lambda - Add a \u0026ldquo;Manual Approval\u0026rdquo; step before production deployment 13/11/2025 13/11/2025 6 - Test flow: Push code -\u0026gt; Auto Build -\u0026gt; Auto Deploy. 14/11/2025 14/11/2025 Week 10 Achievements: Understood the standard DevOps process.\nSuccessfully built an automated pipeline: simply commit code, and the system automatically builds and deploys the new version.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/2-proposal/","title":"Proposal","tags":[],"description":"","content":"E-commerce Platform to Sell Meals Bấm để tải về báo cáo (.docx)\n1. Executive Summary The Personalized Food Ingredient Sales Platform focuses on enabling faster and more efficient shopping. Users register accounts to access a diverse recipe database, receive AI-driven meal suggestions based on purchase history, and order with doorstep delivery. Leveraging AWS cloud infrastructure, the platform ensures flexible scalability, high performance, and secure management.\n2. Problem Statement Current Problem\nCustomers often spend a lot of time searching for suitable meals to buy for their daily needs. While many platforms provide meal or menu recommendations, most of them do not support purchasing complete, ready-made dishes, forcing users to manually search for restaurants or vendors that offer those meals.\nSolution\nThe platform uses Spring Boot to build a stable backend with REST APIs for user accounts, recipes, shopping carts, and orders. The frontend is built with React and provides easy-to-use AI-based meal recommendations. Data is stored in AWS RDS (PostgreSQL), while images and static files are stored in Amazon S3. The backend runs on Amazon EC2 inside a secure VPC, and Route 53 is used for domain management.\nBenefits \u0026amp; Return on Investment\nThis solution establishes a comprehensive platform for a nutrition-focused startup to expand its services while collecting user data for advanced recommendation systems. The cost is 119.51 USD/month and 1,434.12 USD/12 months. The development process leverages open-source frameworks, avoiding additional hardware costs.\n3. Solution Architecture The website is hosted on EC2. Data is stored on an EC2 instance. Images are stored in S3. Code is pushed to GitHub for management and automatically uploaded to S3 so CodeDeploy can deploy the application to the server. CloudFront is used to improve loading performance. Cognito manages user identities. CloudTrail monitors and stores activity logs. CloudWatch monitors and manages performance and the health of AWS resources and applications. IAM grants permissions to services. Secrets Manager stores sensitive information.\nAWS Services Used\nWAF: Protects the web application from cyber-attacks. AWS CloudFront: Improves website loading speed. AWS EC2: Hosts the application, NAT instance, and database. AWS VPC: Virtual private network. AWS S3: Stores code, log files, and images. CodeDeploy: Deploys code to EC2. GitLab: Hosts source code and pushes it to S3. Amazon Cognito: Manages user authentication for the web application. IAM: Creates users and roles. Secrets Manager: Stores sensitive information. CloudTrail: Monitors and stores activity logs. CloudWatch: Monitors and manages the performance and health of AWS resources. 4. Technical Deployment Deployment Phases\nThis project includes two main parts: developing the Spring Boot backend and the React frontend, and deploying the website on AWS using AWS services. Each part includes four phases.\nTheory \u0026amp; Architecture Design: Gather web application requirements, design the system architecture (Spring Boot REST API + React frontend), and define the database schema. (January)\nDevelopment \u0026amp; Testing: Implement the Spring Boot backend with REST APIs (authentication, user management, meal/recipe CRUD, shopping cart, etc.) and build the React frontend (UI/UX, routing, forms, state management). Conduct unit tests for backend services, integration tests for API endpoints, and frontend tests (Jest/React Testing Library). (January–February)\nCost Estimation \u0026amp; Feasibility Check: Use the AWS Pricing Calculator to estimate costs for EC2 (backend hosting), RDS (database), S3 (static files and images), VPC (networking), and Route 53 (domain). Adjust as needed. (February)\nAWS Integration: Integrate AWS services into the application. Deploy the website on EC2, store images on S3, configure RDS for the database, use VPC for networking, Route 53 for domain management, and set up CI/CD pipelines (GitHub Actions or AWS CodePipeline). Perform staging tests before official release. (March)\nTechnical Requirements\nBackend (Spring Boot): REST APIs for authentication, user management, meal/recipe CRUD, shopping cart, and order processing. Includes security (JWT, Spring Security). Frontend (React): Responsive web application with a user-friendly UI/UX integrated with the backend API. Database (EC2): Relational database (MySQL/PostgreSQL) hosted on EC2, storing users, recipes, shopping carts, and order data. Storage (AWS S3): Used to store user-uploaded images. Hosting \u0026amp; Networking (AWS EC2 \u0026amp; AWS VPC): Application deployed on EC2 instances. CI/CD (GitHub Actions or AWS CodePipeline): Automated build and deployment pipeline for backend and frontend. Authentication \u0026amp; Security: JWT authentication and HTTPS configuration; optional AWS Cognito for user access management. 5. Roadmap \u0026amp; Deployment Milestones January: Build theoretical foundation and draw architecture (Spring Boot backend + React frontend design, database schema). Begin initial backend and frontend development. February: Continue backend and frontend development, perform unit and integration tests. Use AWS Pricing Calculator to evaluate hosting costs and refine architecture for cost efficiency. March: Integrate AWS services, configure CI/CD pipelines, conduct staging tests, and deploy the website to production. Post-launch: Up to 3 months for maintenance, optimization, and feature improvements. 6. Cost Estimate Costs can be viewed via the AWS Pricing Calculator.\nAWS Services AWS WAF: $11.6/month Application Load Balancer (ALB): $18.63/month Amazon EC2 Application: $19.27/month Amazon EC2 Data Tier: $9.64/month Amazon EC2 NAT Instances: $19.27/month Amazon S3: $3.72/month AWS CodeDeploy: $0 AWS Secrets Manager: $0.4/month Amazon Cognito: $14.25/month Amazon CloudWatch: $4.91/month AWS CloudTrail: $1.77/month VPC Endpoints: $16.05/month Total: $119.51/month, $1,434.12/year\n7. Risk Assessment Risk Matrix\nNetwork loss: Medium impact / Medium probability. EC2 / ALB / AZ failure: High impact / Low–medium probability. Secret leakage: High impact / Low–medium probability. Cost overrun: Medium impact / Medium probability. Mitigation Strategies\nAvailability: Multi-AZ + Auto Scaling + ALB; health checks; caching with CloudFront; Route 53 failover. Security: IAM with least-privilege; Secrets Manager with rotation and access logging; WAF/Shield protection. Cost Management: AWS Budgets \u0026amp; Cost Explorer; rightsizing; Reserved or Spot Instances where appropriate. Fallback Plan\nAutomatic rollback using CodeDeploy. Manual fallback: GitLab runner → prebuilt AMIs or ASGs. 8. Expected Outcomes Fully automated CI/CD (GitLab → CodePipeline → CodeBuild → CodeDeploy) reduces manual errors. Multi-layer security (IAM, WAF, Secrets Manager) with auditing via CloudTrail. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in four events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Cloud Day\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Event AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: Event AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: Event AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 09:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/5-workshop/5.5-testing/","title":"Testing","tags":[],"description":"","content":"Create input file On your computer, create a file named test.txt. Create with the text: \u0026ldquo;Hello, congratulations on completing the workshop!\u0026rdquo; Save file Return to the S3 tab, open the input folder. Click Upload -\u0026gt; Select test.txt -\u0026gt; Click Upload Wait for about 5 seconds.\nGo back to the Bucket console, then open the output folder. You will see the file test.mp3. Select it and click Download.\nOpen it to listen.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nAutomated Text-to-Speech Converter using Serverless Overview Event-Driven Architecture allows you to build applications that automatically respond to changes in state, such as a file upload, without provisioning or managing servers.\nIn this lab, you will learn how to create, configure, and test a Serverless pipeline that automatically converts text files into lifelike speech audio using AWS Managed Services.\nYou will utilize two key mechanisms to process data asynchronously:\nS3 Event Notifications - Configure Amazon S3 to act as an event source. It will automatically trigger a compute function whenever a new object is uploaded to a specific input folder. Managed AI Services - Leverage Amazon Polly to synthesize speech from text. This allows you to add deep learning capabilities to your application through simple API calls, without needing data science expertise. Content Workshop overview Prerequisite Build Core Logic (Lambda) Configure Storage (S3) Testing Result Clean up "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for building Serverless Event-driven applications on AWS.\nBy configuring S3 Event Notifications, you enabled an automated workflow where compute resources (AWS Lambda) react instantly to data ingestion without the need for manual intervention or server management.\nBy integrating Amazon Polly, you successfully leveraged Managed AI Services to transform text into lifelike speech, demonstrating how to add complex machine learning capabilities to your application with minimal code.\nClean up Delete S3 buckets Open S3 console Open s3-demo-bucket Choose 2 folders input and output Click delete and confirm Then choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. Delete Lambda function Open the lambda console Find workshop1 and click action Choose Delete and confirm Delete IAM role Open IAM console Select Roles from the menu on the left. Find the role PollyLambdaRole. Select the role and click Delete. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at First Cloud Journey program from 08/09/2025 to 09/12/2025, I had the opportunity to learn about cloud technologies and apply what I learned to various projects. I explored AWS services and joined workshops, which helped me gain a deeper understanding of Cloud. Additionally, I had the chance to connect with more people.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Enhance professional skills and practical knowledge Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe work environment is friendly, and the mentors are sociable and supportive. They are always willing to help me if I have any problems.The workspace is tidy and comfortable, helping me focus better. However, I think it would be better to have more social gatherings or team bonding activities on a regular basis to strengthen the relationship.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company often provides me with opportunities to connect with other mentors to improve my networking. Additionally, I\u0026rsquo;ve learned communication skills – which has helped me become more confident in communicating with colleagues, something I wasn\u0026rsquo;t able to do before.\nAdditional Questions What did you find most satisfying during your internship? What I was most satisfied with was the working environment of the company and the friendly mentors during my internship at the company What do you think the company should improve for future interns? I think the company should organize more networking events to learn more about new technologies in the future. If recommending to a friend, would you suggest they intern here? Why or why not? Sure. Because the positive and flexible environment is suitable for those who are just starting to get acquainted with the job and exposed to many new technologies. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? I have no suggestions. Would you like to continue this program in the future? If given the opportunity to work with the company, I think I would continue and pursue AWS certifications. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/tags/","title":"Tags","tags":[],"description":"","content":""}]