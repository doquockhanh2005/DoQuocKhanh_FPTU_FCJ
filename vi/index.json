[{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/","title":"Nhật ký công việc","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTrong trang này bạn sẽ cần giới thiệu worklog của bạn như thế nào? Bạn hoàn thành chương trình trong vòng bao nhiêu tuần? Bạn đã làm gì trong các tuần đó?\nThông thường và cũng là tiêu chuẩn, một worklog được thực hiện trong khoảng 3 tháng (trong suốt thời gian thực tập) với nội dung các tuần như sau:\nTuần 1: Làm quen với AWS, IAM \u0026amp; Billing\nTuần 2: Networking: Kiến trúc VPC, Subnet, Route Table \u0026amp; Security Group\nTuần 3: Compute \u0026amp; Scaling: EC2, EBS, Load Balancer \u0026amp; Auto Scaling\nTuần 4: Storage \u0026amp; CDN: S3, Static Website Hosting \u0026amp; CloudFront\nTuần 5: Database: RDS (SQL), DynamoDB (NoSQL) \u0026amp; Multi-AZ\nTuần 6: Tổng ôn tập \u0026amp; Thực hành Lab tổng hợp (VPC + EC2 + RDS + S3)\nTuần 7: Serverless: Lambda Function \u0026amp; API Gateway\nTuần 8: Containers: Docker, ECR \u0026amp; Amazon ECS Fargate\nTuần 9: Infrastructure as Code (IaC) với AWS CDK\nTuần 10: DevOps: Xây dựng CI/CD Pipeline (CodeBuild, CodePipeline)\nTuần 11: Giám sát hệ thống \u0026amp; Ghi logs với CloudWatch \u0026amp; CloudTrail\nTuần 12: Tổng kết: Tích hợp Serverless, Container \u0026amp; DevOps, workshop 1\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/5-workshop/5.3-s3/create-s3/","title":"Tạo S3","tags":[],"description":"","content":"Tạo S3 Truy cập vào S3\nNhấn chọn Create Bucket:\nTại giao diện Create Bucket: Chỉ định tên cho bucket: s3-demo-text\nKhông thêm tag cho VPC endpoint vào lúc này. Nhấn Create bucket Sau đó chọn bucket vừa tạo rồi chọn Create folder Trong bảng điều khiển Tạo thư mục: Chỉ định tên của bucket: input sau đó chọn Create folder Chúng ta làm tương tự khi tạo tệp output. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/5-workshop/5.1-workshop-overview/","title":"Tổng quan Workshop","tags":[],"description":"","content":"GIỚI THIỆU Giới thiệu về S3 Event Notifications S3 Event Notifications là tính năng cho phép Amazon S3 tự động gửi thông báo khi có các sự kiện cụ thể xảy ra trong Bucket (ví dụ: khi một file mới được upload, xóa hoặc sao chép). S3 có thể gửi thông báo đến nhiều đích đến khác nhau như AWS Lambda, Amazon SNS, hoặc Amazon SQS. Trong workshop này, chúng ta sử dụng sự kiện PutObject để tự động kích hoạt Lambda function ngay khi file văn bản được upload lên S3. Giới thiệu về Amazon Polly Amazon Polly là dịch vụ chuyển văn bản thành giọng nói (Text-to-Speech) sử dụng công nghệ Deep Learning tiên tiến của AWS. Polly hỗ trợ hơn 60 giọng nói trong hơn 20 ngôn ngữ, bao gồm cả tiếng Việt, với chất lượng âm thanh tự nhiên giống con người. Dịch vụ này hoàn toàn được quản lý (fully managed), bạn không cần lo lắng về việc quản lý hạ tầng hay khả năng mở rộng. Tổng quan về workshop Trong workshop này, bạn sẽ xây dựng một ứng dụng Serverless Text-to-Speech Converter (Chuyển đổi văn bản thành giọng nói). Hệ thống hoạt động hoàn toàn tự động dựa trên mô hình Event-driven Architecture:\nAmazon S3 (Input Bucket): Lưu trữ file văn bản đầu vào (.txt) do người dùng upload. S3 Event Notifications: Phát hiện sự kiện upload file mới và kích hoạt AWS Lambda. AWS Lambda: Bộ xử lý trung tâm, đóng vai trò điều phối luồng dữ liệu giữa S3 và Polly. Lambda đọc file văn bản, gọi Polly API để chuyển đổi, và lưu kết quả. Amazon Polly: Dịch vụ AI thực hiện chuyển đổi văn bản thành âm thanh với giọng nói tự nhiên. Amazon S3 (Output Bucket): Lưu trữ file âm thanh đầu ra (.mp3) sau khi xử lý hoàn tất. Toàn bộ quy trình diễn ra tự động mà không cần can thiệp thủ công, giúp tiết kiệm thời gian và chi phí vận hành.\nKiến trúc hệ thống Mô hình dưới đây mô tả chi tiết kiến trúc và luồng dữ liệu (Data Flow) của hệ thống Text-to-Speech Converter:\nLuồng hoạt động (Workflow):\nUpload file: Người dùng tải file văn bản (.txt) lên Amazon S3 Input Bucket.\nEvent trigger: S3 phát hiện sự kiện PutObject (có file mới), tự động gửi event notification kích hoạt AWS Lambda Function.\nLambda xử lý: Lambda function được trigger và thực hiện:\nĐọc nội dung file văn bản từ S3 Input Bucket Gửi nội dung văn bản đến Amazon Polly API cùng với các tham số (voice ID, output format) Polly chuyển đổi: Amazon Polly nhận request, xử lý văn bản và trả về audio stream (dữ liệu âm thanh) cho Lambda.\nLưu kết quả: Lambda nhận audio stream từ Polly và lưu dưới dạng file .mp3 vào Amazon S3 Output Bucket.\nHoàn tất: Người dùng có thể tải xuống file MP3 từ S3 Output Bucket để sử dụng.\nLợi ích của kiến trúc Serverless Không cần quản lý server: AWS tự động xử lý scaling, patching và high availability. Chi phí tối ưu: Chỉ trả tiền khi có request xử lý (pay-per-use model). Tự động mở rộng: Hệ thống có thể xử lý từ vài requests đến hàng triệu requests mà không cần cấu hình thêm. Triển khai nhanh: Tập trung vào logic ứng dụng thay vì quản lý infrastructure. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/4-eventparticipated/4.4-event4/","title":"Event AWS Cloud Mastery Series #3","tags":[],"description":"","content":"Báo cáo Tổng hợp: “Identity \u0026amp; Access Management” Mục tiêu Sự kiện Hiểu rõ về IAM, quản lý thông tin xác thực và cấu trúc AWS Organization. Làm chủ hệ thống phát hiện mối đe dọa với GuardDuty và tư duy Detection-as-Code. Thiết kế kiến trúc bảo mật mạng phân lớp (Layered Security) và triển khai Network Firewall tối ưu. Hiểu sâu về cơ chế mã hóa dữ liệu với AWS KMS (Key Management Service): Sự khác biệt giữa AWS Managed Key và Customer Managed Key (CMK). Áp dụng chiến lược lọc traffic tự động (Automated Domain Lists) để chặn các mối đe dọa mới. Speakers FCJ members Điểm nhấn Chính Quản lý Danh tính \u0026amp; Truy cập (IAM) Nguyên tắc: Loại bỏ Access Keys dài hạn, sử dụng Short-term credentials qua IAM Identity Center. Kiểm soát: Kết hợp SCP (chặn quyền mức tổ chức) và Permission Boundaries (giới hạn quyền mức user). Tự động hóa: Xoay vòng mật khẩu DB tự động với Secrets Manager. Phát hiện \u0026amp; Phản ứng (Detection \u0026amp; Response) GuardDuty: Giám sát thời gian thực (Runtime Monitoring) để phát hiện hành vi bất thường sâu trong OS (process, file access). Detection-as-Code: Quản lý quy tắc phát hiện dưới dạng mã nguồn (CloudFormation/Terraform) để đảm bảo tính nhất quán và tuân thủ (Compliance). An ninh Mạng (Network Security) Bảo mật đa lớp: Kết hợp WAF (Layer 7) -\u0026gt; Network Firewall (Layer 3-7) -\u0026gt; NACL (Subnet) -\u0026gt; Security Group (Instance). Network Firewall: Sử dụng Active Threat Defense để tự động cập nhật rules chặn các mối đe dọa mới nhất từ AWS Threat Intelligence. Tính năng Automated Domain Lists: Tự động phân tích traffic HTTP/HTTPS để tạo rule chặn hoặc cho phép dựa trên tên miền (FQDN) mà không cần quản lý IP thủ công. Bảo vệ Dữ liệu (Data Protection) Cơ chế KMS: KMS quản lý Master Key (không bao giờ rời khỏi KMS). Master Key dùng để mã hóa Data Key. Data Key (dạng plaintext) mới thực sự được dùng để mã hóa dữ liệu người dùng. Phân loại Key: AWS Managed Key: Miễn phí, do AWS quản lý, user không thể xoay vòng (rotate) thủ công hay thay đổi policy. Customer Managed Key (CMK): Có phí, user toàn quyền kiểm soát (xoay vòng, phân quyền, xóa), bắt buộc dùng cho các yêu cầu tuân thủ cao. Mã hóa EBS: Quy trình mã hóa ổ cứng EBS bao gồm việc tạo Data Key, mã hóa volume bằng Data Key đó và lưu trữ Data Key đã mã hóa kèm theo volume. Bài học Chính (Key Takeaways) Chiến lược Bảo vệ Dữ liệu Sử dụng CMK cho dữ liệu nhạy cảm: Mặc dù AWS Managed Key tiện lợi, nhưng CMK cho phép kiểm soát chặt chẽ hơn về việc ai được phép dùng key để giải mã (thông qua Key Policy). Key Rotation: Bật tính năng tự động xoay vòng key mỗi năm (đối với CMK) để đảm bảo an toàn nếu key cũ bị lộ. Kiến trúc Kỹ thuật Tối ưu chi phí Network: Sử dụng mô hình Multiple VPC Endpoints cho Network Firewall để giảm chi phí vận hành và đơn giản hóa kiến trúc mạng. Security Group Reference: Thay vì whitelist IP cứng trong Security Group, hãy tham chiếu đến SG ID của các tier khác (ví dụ: SG-App chỉ cho phép traffic từ SG-Web) để linh hoạt hơn. Ứng dụng vào Project Cải thiện Quản lý Danh tính (IAM) Audit \u0026amp; Clean-up: Rà soát lại toàn bộ IAM User trong dự án, xóa các Access Key cũ/không sử dụng. Bắt buộc kích hoạt MFA (ưu tiên FIDO2/YubiKey) cho tất cả tài khoản. Implement Secrets Manager: Thay thế các biến môi trường chứa password DB trong code (ví dụ Spring Boot/Node.js) bằng code gọi API lấy secret từ AWS Secrets Manager. Tăng cường An ninh Mạng Chặn luồng ra (Egress Filtering): Triển khai AWS Network Firewall hoặc DNS Firewall để kiểm soát traffic từ server ra Internet, ngăn chặn kết nối đến các domain lạ hoặc C2 server. Review Security Groups: Chuyển đổi các rule đang dùng IP tĩnh sang dùng Security Group Referencing để hỗ trợ tốt hơn cho Auto Scaling. Giám sát \u0026amp; Phản ứng Tự động Kích hoạt GuardDuty: Bật GuardDuty trên môi trường Production để phát hiện ngay các hành vi bất thường (như đào tiền ảo, port scanning). Automated Remediation: Viết một Lambda function đơn giản và kết nối với EventBridge: Khi GuardDuty báo mức độ \u0026ldquo;High\u0026rdquo;, tự động thu hồi session token hoặc chặn Security Group của instance đó. Trải nghiệm Sự kiện Tham gia chuyên đề “AWS Mastery #3” đã giúp tôi hệ thống hóa lại toàn bộ các lớp bảo mật trên AWS.Là một lập trình viên thường tập trung vào code hơn là hạ tầng, sự kiện này đã thực sự thay đổi tư duy của tôi về trách nhiệm bảo mật:\nThay đổi tư duy về \u0026ldquo;Code sạch\u0026rdquo; Trước đây, tôi nghĩ không hardcode password là đủ. Nhưng khi thấy demo về IAM Access Analyzer, tôi nhận ra việc viết Infrastructure-as-Code (Terraform/CloudFormation) cũng cần phải được \u0026ldquo;lint\u0026rdquo; (kiểm tra lỗi) về logic bảo mật. Một dòng Principal: * vô tình trong code có thể mở toang cửa cho hacker, và công cụ này hoạt động như một \u0026ldquo;Unit Test\u0026rdquo; cho policy vậy. Phân tích mã độc Mélofee Nhìn thấy dòng lệnh wget http://173.209\u0026hellip; kết nối trực tiếp đến IP thay vì domain đã làm tôi \u0026ldquo;tỉnh ngộ\u0026rdquo;. Tôi từng nghĩ chỉ cần chặn DNS là an toàn, nhưng malware thực tế khôn ngoan hơn nhiều. Điều này chứng minh tại sao Network Firewall (chặn IP egress) lại quan trọng đến thế đối với các server backend. Bài học \u0026ldquo;xương máu\u0026rdquo; về chi phí Một chi tiết nhỏ nhưng đắt giá mà diễn giả chia sẻ: Tài khoản Free Tier sẽ mất hiệu lực ngay lập tức khi tham gia AWS Organization. Đây là thông tin cực kỳ hữu ích cho các anh em Dev hay dùng tài khoản cá nhân để vọc vạch, giúp tránh được những hóa đơn \u0026ldquo;từ trên trời rơi xuống\u0026rdquo;. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/4-eventparticipated/4.3-event3/","title":"Event AWS Cloud Mastery Series #2","tags":[],"description":"","content":"Báo cáo tổng hợp: “DevOps on AWS” Mục tiêu Sự kiện Phân tích sự chuyển dịch từ thao tác thủ công (ClickOps) sang tự động hóa hạ tầng (IaC) để loại bỏ sai sót con người và sự thiếu nhất quán. Nắm vững cơ chế hoạt động của AWS CloudFormation trong việc định nghĩa và quản lý vòng đời tài nguyên đám mây . Làm chủ AWS CDK (Cloud Development Kit) để xây dựng hạ tầng bằng ngôn ngữ lập trình bậc cao thông qua các cấp độ Construct . Hiểu sâu về công nghệ Docker, quy trình đóng gói ứng dụng và quản lý image với Amazon ECR. So sánh và lựa chọn giải pháp điều phối Container phù hợp giữa Amazon ECS, Amazon EKS và AWS App Runner. Speakers FCJ members Điểm nhấn Chính Tư duy Hạ tầng \u0026amp; Quản trị Cấu hình Hạn chế của ClickOps: Thao tác trên giao diện điều khiển (Console) gây ra rủi ro cao về lỗi con người, tốc độ chậm và khó khăn trong cộng tác. CloudFormation Stack: Quản lý tập hợp các tài nguyên như một đơn vị duy nhất, cho phép tạo, cập nhật hoặc xóa toàn bộ hạ tầng một cách đồng bộ. Phát hiện sai lệch (Drift Detection): Cơ chế quan trọng giúp nhận diện khi cấu hình thực tế của tài nguyên bị thay đổi thủ công và không còn khớp với template định nghĩa ban đầu. Lập trình Hạ tầng với AWS CDK Đa ngôn ngữ: Hỗ trợ định nghĩa hạ tầng bằng TypeScript, JavaScript, Python, Java, C#/.NET và Go. Hệ thống Constructs (Cấu trúc): L1 Constructs: Ánh xạ 1:1 với tài nguyên CloudFormation (Cfn), yêu cầu cấu hình chi tiết. L2 Constructs: Cung cấp các thiết lập mặc định (defaults) và các phương thức hỗ trợ (helper methods) tối ưu. L3 Constructs (Patterns): Các mẫu kiến trúc hoàn chỉnh, kết hợp nhiều tài nguyên để giải quyết một bài toán cụ thể. Quy trình làm việc (Workflow): Từ khởi tạo (cdk init), tổng hợp template (cdk synth), đến triển khai (cdk deploy) và hủy bỏ (cdk destroy). Hệ sinh thái Container \u0026amp; Điều phối Docker Fundamentals: Phân biệt Container (nhẹ, chia sẻ kernel) với Virtual Machine (nặng, có Guest OS) và quy trình Build - Push - Pull - Run. Amazon ECR: Kho lưu trữ container image bảo mật, hỗ trợ quét lỗ hổng (scanning) và các chính sách vòng đời (lifecycle policies). Các mô hình điều phối (Orchestration): Amazon ECS: Giải pháp native của AWS, đơn giản, tích hợp sâu, hỗ trợ chạy trên EC2 hoặc Fargate (Serverless). Amazon EKS: Dịch vụ Kubernetes được quản lý, cung cấp sự linh hoạt và chuẩn hóa mã nguồn mở nhưng phức tạp hơn trong vận hành. AWS App Runner: Giải pháp PaaS giúp triển khai nhanh ứng dụng web trực tiếp từ mã nguồn hoặc image mà không cần quản lý hạ tầng . Bài học Chính Chiến lược IaC Code thay vì Clicks: Chuyển đổi hoàn toàn sang mô hình \u0026ldquo;No More ClickOps\u0026rdquo; để đảm bảo tính nhất quán và khả năng mở rộng. Kiểm soát phiên bản: Sử dụng template (YAML/JSON) hoặc mã nguồn CDK đóng vai trò là bản thiết kế (blueprint) duy nhất cho hạ tầng. Quản lý sự thay đổi: Thường xuyên sử dụng cdk diff và Drift Detection để kiểm soát sự khác biệt giữa mã nguồn và thực tế trước khi triển khai. Kiến trúc Kỹ thuật Tính bất biến (Immutability): Docker Image và Container giúp đảm bảo ứng dụng chạy nhất quán trên mọi môi trường (\u0026ldquo;It works on my machine\u0026rdquo; -\u0026gt; \u0026ldquo;Ship your machine\u0026rdquo;). Lựa chọn Compute: Sử dụng AWS Fargate để loại bỏ gánh nặng quản lý máy chủ (Serverless), hoặc EC2 khi cần kiểm soát sâu và tối ưu chi phí cho các tác vụ dài hạn. Phân cấp trừu tượng: Tận dụng L2/L3 Constructs trong CDK để giảm lượng code phải viết và thừa hưởng các best practices có sẵn. Ứng dụng vào project Triển khai App Runner: Sử dụng cho các ứng dụng web hoặc API cần triển khai nhanh (prototypes) mà không muốn quản lý server. Tối ưu hóa ECS: Áp dụng mô hình ECS kết hợp với Application Load Balancer (ALB) cho các kiến trúc microservices tiêu chuẩn như trong bài demo \u0026ldquo;Cats vs Dogs\u0026rdquo;. Chuyển đổi sang CDK: Bắt đầu viết hạ tầng bằng TypeScript/Python thay vì CloudFormation thuần để tăng tốc độ phát triển và khả năng tái sử dụng mã. Trải nghiệm Sự kiện Tham gia chuyên đề “AWS Mastery #2” đã cung cấp một góc nhìn sâu sắc về việc hiện đại hóa quy trình vận hành thông qua mã hóa hạ tầng và container. Những trải nghiệm thực tế bao gồm:\nHọc hỏi từ chuyên gia Hiểu rõ sự khác biệt cốt lõi giữa Terraform (đa nền tảng, dùng HCL) và AWS CDK/CloudFormation (tối ưu cho AWS, dùng ngôn ngữ lập trình) để đưa ra lựa chọn công cụ phù hợp. Nắm bắt được cấu trúc cây (Construct Tree) của một ứng dụng CDK, từ App đến Stack và các Resources. Bài học rút ra Lựa chọn công cụ: ECS hoặc App Runner là lựa chọn tốt nhất cho các đội ngũ muốn giảm tải vận hành (lower ops overhead), trong khi EKS dành cho các yêu cầu phức tạp và tính di động cao (portability). Tự động hóa là bắt buộc: Việc tích hợp CI/CD với CodePipeline và CodeBuild là chìa khóa để duy trì sự ổn định khi triển khai các kiến trúc container phân tán. Tiêu chuẩn hóa: Sử dụng Dockerfile và ECR giúp chuẩn hóa môi trường runtime, loại bỏ hoàn toàn các lỗi do sự khác biệt môi trường gây ra. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/1.9-week9/","title":"Worklog Tuần 9","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 9: Chuyển đổi tư duy từ \u0026ldquo;ClickOps\u0026rdquo; (dùng chuột) sang \u0026ldquo;IaC\u0026rdquo; (dùng code). Sử dụng AWS CDK để định nghĩa hạ tầng. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu khái niệm IaC (Infrastructure as Code) - Giới thiệu CloudFormation và AWS CDK (Cloud Development Kit) 03/11/2025 03/11/2025 3 - Thực hành: Cài đặt AWS CDK, khởi tạo project CDK (TypeScript/Python) - Tìm hiểu cấu trúc App, Stack, Construct 04/11/2025 04/11/2025 4 - Thực hành: Viết code CDK để tạo một S3 Bucket và DynamoDB Table - Lệnh cdk synth và cdk deploy 05/11/2025 05/11/2025 5 - Thực hành nâng cao: Viết code CDK để dựng lại VPC (như tuần 2) - Quản lý sự phụ thuộc giữa các resource trong code 06/11/2025 06/11/2025 6 - Thực hiện cdk destroy để dọn dẹp tài nguyên. Hiểu về Drift detection. 07/11/2025 07/11/2025 Kết quả đạt được tuần 9: Hiểu lợi ích của IaC trong việc quản lý version và tái sử dụng hạ tầng.\nCó khả năng dùng ngôn ngữ lập trình để tự động hóa việc tạo tài nguyên AWS thay vì làm thủ công.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/1.8-week8/","title":"Worklog Tuần 8","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 8: Hiểu về Containerization trên AWS. Deploy ứng dụng Docker lên AWS ECS Fargate. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Ôn tập Docker: Dockerfile, Image, Container - Tìm hiểu Amazon ECR (Elastic Container Registry) 27/10/2025 27/10/2025 3 - Thực hành: Build Docker Image cho một app NodeJS đơn giản - Push Image lên ECR repository 28/10/2025 28/10/2025 4 - Tìm hiểu Amazon ECS (Elastic Container Service) - Phân biệt EC2 Launch Type và Fargate (Serverless compute for containers) 29/10/2025 29/10/2025 5 - Thực hành: Tạo ECS Cluster, Task Definition và Service - Deploy container chạy trên nền tảng Fargate 30/10/2025 30/10/2025 6 - Kiểm tra ứng dụng chạy, xem logs container trên AWS. 31/10/2025 31/10/2025 Kết quả đạt được tuần 8: Biết cách đóng gói ứng dụng và quản lý Docker Images trên ECR.\nĐã deploy ứng dụng container hóa lên môi trường production-ready sử dụng ECS Fargate mà không cần quản lý server vật lý.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/1.7-week7/","title":"Worklog Tuần 7","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 7: Làm quen với khái niệm Serverless. Xây dựng REST API hoàn toàn không dùng máy chủ (serverless). Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu AWS Lambda: Runtime, Handler, Trigger - Các giới hạn của Lambda (Timeout, Memory) 20/10/2025 20/10/2025 3 - Thực hành: Viết Lambda Function đơn giản (Hello World) bằng Python/NodeJS - Test Lambda function với test event 21/10/2025 21/10/2025 4 - Tìm hiểu Amazon API Gateway: Resources, Methods, Stages - Tích hợp API Gateway với Lambda 22/10/2025 22/10/2025 5 - Thực hành: Xây dựng API CRUD đơn giản (Create, Read, Update, Delete) - Lưu dữ liệu từ Lambda vào DynamoDB 6 - Test API bằng Postman/Curl. Theo dõi logs qua CloudWatch. 24/10/2025 24/10/2025 Kết quả đạt được tuần 7: Hiểu cách vận hành code mà không cần quản lý hạ tầng server.\nĐã xây dựng thành công Serverless API Backend sử dụng API Gateway, Lambda và DynamoDB.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/1.6-week6/","title":"Worklog Tuần 6","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 6: Củng cố và hệ thống hóa kiến thức từ Tuần 1 đến Tuần 5. Tự triển khai một mô hình kết hợp (VPC + EC2 + RDS + S3) mà hạn chế nhìn tài liệu. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Ôn tập Tuần 1 \u0026amp; 2: IAM, Billing và VPC (quan trọng). - Vẽ lại sơ đồ mạng VPC đã triển khai để kiểm tra độ hiểu về Routing/Subnet. 13/10/2025 13/10/2025 3 - Ôn tập Tuần 3: EC2, Load Balancer, Auto Scaling. - Review lại các bước debug khi không SSH được vào server. 14/10/2025 14/10/2025 4 - Ôn tập Tuần 4 \u0026amp; 5: S3, CloudFront, RDS, DynamoDB. - So sánh lại ưu/nhược điểm giữa SQL (RDS) và NoSQL (DynamoDB). 15/10/2025 15/10/2025 5 - Thực hành Lab Tổng hợp: Dựng lại mô hình 2-tier (Web Server kết nối Database) trong Custom VPC. - Cấu hình S3 để lưu trữ file backup từ EC2. 16/10/2025 18/10/2025 6 - Dọn dẹp tài nguyên (Clean up) toàn bộ các lab cũ/thừa để tối ưu chi phí. - Tổng hợp các lỗi thường gặp (Troubleshooting log) vào sổ tay cá nhân. 18/10/2025 18/10/2025 Kết quả đạt được tuần 6: Đã hệ thống hóa lại bức tranh tổng thể về hạ tầng AWS cơ bản (Compute, Network, Storage, Database).\nTự tin triển khai và debug các lỗi kết nối phổ biến trong môi trường VPC.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/1.5-week5/","title":"Worklog Tuần 5","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 5: Phân biệt và sử dụng được RDS (SQL) và DynamoDB (NoSQL). Hiểu về kiến trúc Multi-AZ cho Database. Thực hành: Kết nối ứng dụng với Database. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu Amazon RDS, các engine hỗ trợ (MySQL, Postgres\u0026hellip;) - Khái niệm Multi-AZ và Read Replicas 06/10/2025 06/10/2025 3 - Thực hành: Tạo RDS Instance (MySQL) trong Private Subnet - Cấu hình Security Group cho phép EC2 kết nối tới RDS 07/10/2025 07/10/2025 4 - Tìm hiểu Amazon DynamoDB (NoSQL): Table, Item, Partition Key, Sort Key - Các chế độ Capacity (Provisioned vs On-demand) 08/10/2025 08/10/2025 5 - Thực hành: Tạo bảng DynamoDB, thêm/sửa/xóa item qua AWS Console và AWS CLI - Tìm hiểu ElastiCache (Redis) cơ bản 09/10/2025 09/10/2025 6 - Thực hành: Viết script nhỏ (Python/NodeJS) trên EC2 để query dữ liệu từ RDS và DynamoDB. 10/10/2025 10/10/2025 Kết quả đạt được tuần 5: Đã tạo và cấu hình thành công RDS MySQL an toàn trong Private Subnet.\nHiểu cách thiết kế bảng cơ bản với DynamoDB.\nThực hiện kết nối thành công từ Web Server vào Database Server.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/1.4-week4/","title":"Worklog Tuần 4","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 4: Làm việc với dịch vụ lưu trữ đối tượng S3.\nTăng tốc độ truy cập nội dung tĩnh với CloudFront (CDN).\nCác công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu về Amazon S3: Buckets, Objects, Storage Classes - Tìm hiểu S3 Security (Bucket Policy, ACLs) 29/09/2025 29/09/2025 3 - Thực hành: Tạo S3 Bucket, upload file, bật tính năng Versioning - Cấu hình S3 Static Website Hosting 30/09/2025 30/09/2025 4 - Tìm hiểu về Amazon CloudFront (CDN), Edge Locations - Khái niệm Origin và Behavior trong CloudFront 01/10/2025 01/10/2025 5 - Thực hành: Tạo CloudFront Distribution với Origin là S3 Bucket - Cấu hình OAC (Origin Access Control) để bảo mật S3 02/10/2025 02/10/2025 6 - Ôn tập S3 và CloudFront 03/10/2025 03/10/2025 Kết quả đạt được tuần 4: Hiểu rõ các lớp lưu trữ của S3 để tối ưu chi phí.\nĐã host thành công một website tĩnh (HTML/CSS/JS) trên S3.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/1.3-week3/","title":"Worklog Tuần 3","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 3: Học dịch vụ máy chủ ảo EC2 và các loại ổ đĩa EBS. Hiểu về tính sẵn sàng cao (High Availability) và khả năng mở rộng (Scalability). Thực hành: Deploy web server với Load Balancer và Auto Scaling. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu các dòng EC2 (Instance types), AMI và Key Pairs - Tìm hiểu về EBS Volumes và Snapshots 22/09/2025 22/09/2025 3 - Thực hành: Khởi tạo EC2 Linux, SSH vào server, cài đặt Apache/Nginx đơn giản - Gắn thêm EBS Volume vào EC2 23/09/2025 23/09/2025 4 - Tìm hiểu về Elastic Load Balancer (ALB vs NLB) - Tìm hiểu về Auto Scaling Group (ASG) và Launch Template 24/09/2025 24/09/2025 5 - Thực hành: Tạo Application Load Balancer trỏ vào EC2 - Cấu hình Target Groups và Health Checks 25/09/2025 25/09/2025 6 - Thực hành Lab: Giả lập traffic tăng cao để Auto Scaling tự động tạo thêm server mới. 26/09/2025 26/09/2025 Kết quả đạt được tuần 3: Đã deploy thành công một Web Server cơ bản trên EC2.\nHiểu và cấu hình được Load Balancer để phân phối tải.\nThiết lập được Auto Scaling Group để hệ thống tự động co giãn theo nhu cầu thực tế.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/4-eventparticipated/4.1-event1/","title":"Event CloudDay","tags":[],"description":"","content":"Bài thu hoạch “Building a Unified Data Foundation on AWS for AI and Analytics Workloads” Mục Đích Của Sự Kiện Chia sẻ chiến lược và các phương pháp tối ưu (best practices) để xây dựng một nền tảng dữ liệu hợp nhất, có khả năng mở rộng. Tùy chỉnh hạ tầng dữ liệu để hỗ trợ các khối lượng công việc (workloads) về AI và phân tích. Tận dụng các dịch vụ của AWS để tạo ra một môi trường vững chắc cho các ứng dụng hiện đại. Bao quát các thành phần then chốt bao gồm: thu thập (ingestion), lưu trữ, xử lý và quản trị dữ liệu. Kích hoạt khả năng quản lý dữ liệu hiệu quả phục vụ cho các sáng kiến phân tích nâng cao và AI. Danh Sách Diễn Giả Kien Nguyen - Solutions Architect (Kiến trúc sư Giải pháp), AWS Nội Dung Nổi Bật Sự trỗi dậy của Agentic AI Chuyển dịch từ Chatbot -\u0026gt; Hệ thống Agentic (tự chủ, đa tác nhân, mô phỏng logic con người). Dự báo đến năm 2028, 15% các quyết định công việc hàng ngày sẽ được thực hiện tự động bởi AI. Amazon Bedrock AgentCore Security Triển khai các agent an toàn ở quy mô lớn với các kiểm soát cấp doanh nghiệp. Capabilities Tăng cường sức mạnh cho agent bằng các Công cụ (Tools/APIs) để thực hiện hành động và Bộ nhớ (Memory) để lưu giữ ngữ cảnh khách hàng. Observability: Tầm nhìn toàn diện để giám sát quá trình suy luận và hành động của agent. Bài học Cốt lõi Tư duy Chiến lược Phương pháp 4 bước: Xác định domain events → sắp xếp timeline → identify actors → xác định bounded contexts Case study bookstore: Minh họa cách áp dụng DDD thực tế Context mapping: 7 patterns tích hợp bounded contexts Event-Driven Architecture Tự chủ hơn là Tự động hóa: Mục tiêu là xây dựng các hệ thống có thể độc lập đạt được mục tiêu, thay vì chỉ tự động hóa các tác vụ lặp lại đơn thuần. Nhận thức Rủi ro: Đổi mới sáng tạo phải đi đôi với các kiểm soát rủi ro và xác định rõ giá trị kinh doanh. Kiến Trúc Kỹ Thuật Vector Search (Tìm kiếm Vector): Là xương sống của bộ nhớ AI; thành phần thiết yếu cho các quy trình RAG (Retrieval-Augmented Generation) Hệ thống Đa tác nhân (Multi-Agent Systems): Tương lai nằm ở sự phối hợp giữa các agent chuyên biệt (ví dụ: Agent Giám sát kết hợp với Agent trả lời câu hỏi thường gặp - FAQ). Ứng Dụng Vào Công Việc Đánh giá độ sẵn sàng của Dữ liệu Kiểm tra khả năng tương thích của các cơ sở dữ liệu hiện tại với Vector Search để hỗ trợ bộ nhớ cho AI Xác định Kiểm soát Rủi ro: Thiết lập các chính sách quản trị cho các câu lệnh (prompts) và phản hồi của AI trước khi mở rộng quy mô Thử nghiệm Bedrock: Bắt đầu thử nghiệm với AgentCore để xây dựng một bản mẫu (prototype) tích hợp các công cụ đơn giản. Trải nghiệm trong event Tham gia workshop “Building a Unified Data Foundation on AWS for AI and Analytics Workloads” là một trải nghiệm rất bổ ích, giúp tôi có cái nhìn toàn diện về cách hiện đại hóa ứng dụng và cơ sở dữ liệu bằng các phương pháp và công cụ hiện đại. Một số trải nghiệm nổi bật:\nHọc hỏi từ thị trường Hiểu rõ tính cấp thiết của việc áp dụng công nghệ: Dự báo 33% phần mềm sẽ sử dụng Agentic AI vào năm 2028 Nắm bắt lý do tại sao 40% các dự án có thể thất bại (do chi phí, kiểm soát rủi ro kém) và cách phòng tránh những cạm bẫy này. Trực quan hóa kỹ thuật Hình dung được kiến trúc của Hệ thống Agentic: Cách các agent nhận thức, suy luận và hành động tự chủ. Thấy rõ mối liên kết trực tiếp giữa Chiến lược Dữ liệu (Cơ sở dữ liệu Vector) và Năng lực của Agent (Bộ nhớ). "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/","title":"Báo cáo thực tập","tags":[],"description":"","content":"Báo cáo thực tập Thông tin sinh viên: Họ và tên: Đỗ Quốc Khánh\nSố điện thoại: 0862174951\nEmail: doquockhanhcntt@gmail.com\nTrường: FPT University TP.HCM\nNgành: Công nghệ thông tin\nMSSV: SE194843\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: FCJ Cloud Intern\nThời gian thực tập: Từ ngày 08/09/2025 đến ngày 09/12/2025\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nBắt đầu với healthcare data lakes: Sử dụng microservices Các data lake có thể giúp các bệnh viện và cơ sở y tế chuyển dữ liệu thành những thông tin chi tiết về doanh nghiệp và duy trì hoạt động kinh doanh liên tục, đồng thời bảo vệ quyền riêng tư của bệnh nhân. Data lake là một kho lưu trữ tập trung, được quản lý và bảo mật để lưu trữ tất cả dữ liệu của bạn, cả ở dạng ban đầu và đã xử lý để phân tích. data lake cho phép bạn chia nhỏ các kho chứa dữ liệu và kết hợp các loại phân tích khác nhau để có được thông tin chi tiết và đưa ra các quyết định kinh doanh tốt hơn.\nBài đăng trên blog này là một phần của loạt bài lớn hơn về việc bắt đầu cài đặt data lake dành cho lĩnh vực y tế. Trong bài đăng blog cuối cùng của tôi trong loạt bài, “Bắt đầu với data lake dành cho lĩnh vực y tế: Đào sâu vào Amazon Cognito”, tôi tập trung vào các chi tiết cụ thể của việc sử dụng Amazon Cognito và Attribute Based Access Control (ABAC) để xác thực và ủy quyền người dùng trong giải pháp data lake y tế. Trong blog này, tôi trình bày chi tiết cách giải pháp đã phát triển ở cấp độ cơ bản, bao gồm các quyết định thiết kế mà tôi đã đưa ra và các tính năng bổ sung được sử dụng. Bạn có thể truy cập các code samples cho giải pháp tại Git repo này để tham khảo.\nHướng dẫn kiến trúc Thay đổi chính kể từ lần trình bày cuối cùng của kiến trúc tổng thể là việc tách dịch vụ đơn lẻ thành một tập hợp các dịch vụ nhỏ để cải thiện khả năng bảo trì và tính linh hoạt. Việc tích hợp một lượng lớn dữ liệu y tế khác nhau thường yêu cầu các trình kết nối chuyên biệt cho từng định dạng; bằng cách giữ chúng được đóng gói riêng biệt với microservices, chúng ta có thể thêm, xóa và sửa đổi từng trình kết nối mà không ảnh hưởng đến những kết nối khác. Các microservices được kết nối rời thông qua tin nhắn publish/subscribe tập trung trong cái mà tôi gọi là “pub/sub hub”.\nGiải pháp này đại diện cho những gì tôi sẽ coi là một lần lặp nước rút hợp lý khác từ last post của tôi. Phạm vi vẫn được giới hạn trong việc nhập và phân tích cú pháp đơn giản của các HL7v2 messages được định dạng theo Quy tắc mã hóa 7 (ER7) thông qua giao diện REST.\nKiến trúc giải pháp bây giờ như sau:\nHình 1. Kiến trúc tổng thể; những ô màu thể hiện những dịch vụ riêng biệt.\nMặc dù thuật ngữ microservices có một số sự mơ hồ cố hữu, một số đặc điểm là chung:\nChúng nhỏ, tự chủ, kết hợp rời rạc Có thể tái sử dụng, giao tiếp thông qua giao diện được xác định rõ Chuyên biệt để giải quyết một việc Thường được triển khai trong event-driven architecture Khi xác định vị trí tạo ranh giới giữa các microservices, cần cân nhắc:\nNội tại: công nghệ được sử dụng, hiệu suất, độ tin cậy, khả năng mở rộng Bên ngoài: chức năng phụ thuộc, tần suất thay đổi, khả năng tái sử dụng Con người: quyền sở hữu nhóm, quản lý cognitive load Lựa chọn công nghệ và phạm vi giao tiếp Phạm vi giao tiếp Các công nghệ / mô hình cần xem xét Trong một microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Giữa các microservices trong một dịch vụ AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Giữa các dịch vụ Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The pub/sub hub Việc sử dụng kiến trúc hub-and-spoke (hay message broker) hoạt động tốt với một số lượng nhỏ các microservices liên quan chặt chẽ.\nMỗi microservice chỉ phụ thuộc vào hub Kết nối giữa các microservice chỉ giới hạn ở nội dung của message được xuất Giảm số lượng synchronous calls vì pub/sub là push không đồng bộ một chiều Nhược điểm: cần phối hợp và giám sát để tránh microservice xử lý nhầm message.\nCore microservice Cung cấp dữ liệu nền tảng và lớp truyền thông, gồm:\nAmazon S3 bucket cho dữ liệu Amazon DynamoDB cho danh mục dữ liệu AWS Lambda để ghi message vào data lake và danh mục Amazon SNS topic làm hub Amazon S3 bucket cho artifacts như mã Lambda Chỉ cho phép truy cập ghi gián tiếp vào data lake qua hàm Lambda → đảm bảo nhất quán.\nFront door microservice Cung cấp API Gateway để tương tác REST bên ngoài Xác thực \u0026amp; ủy quyền dựa trên OIDC thông qua Amazon Cognito Cơ chế deduplication tự quản lý bằng DynamoDB thay vì SNS FIFO vì: SNS deduplication TTL chỉ 5 phút SNS FIFO yêu cầu SQS FIFO Chủ động báo cho sender biết message là bản sao Staging ER7 microservice Lambda “trigger” đăng ký với pub/sub hub, lọc message theo attribute Step Functions Express Workflow để chuyển ER7 → JSON Hai Lambda: Sửa format ER7 (newline, carriage return) Parsing logic Kết quả hoặc lỗi được đẩy lại vào pub/sub hub Tính năng mới trong giải pháp 1. AWS CloudFormation cross-stack references Ví dụ outputs trong core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/4-eventparticipated/4.2-event2/","title":"Event AWS Cloud Mastery Series #1","tags":[],"description":"","content":"Báo cáo tổng hợp: “Hội thảo Dịch vụ AWS AI/ML \u0026amp; Generative AI” Mục tiêu Sự kiện Cung cấp cái nhìn tổng quan về bức tranh AI/ML tại Việt Nam. Minh họa quy trình máy học (Machine Learning) toàn diện sử dụng Amazon SageMaker. Giới thiệu các khả năng của Generative AI với Amazon Bedrock (Mô hình nền tảng, Agents, Guardrails). Chia sẻ các kỹ thuật về Kỹ thuật tạo câu lệnh (Prompt Engineering) và Tạo nội dung tăng cường truy xuất (RAG). Diễn giả Các Kiến trúc sư giải pháp (Solutions Architects) Các Chuyên gia về AI/ML Điểm nhấn Chính Tổng quan Dịch vụ AWS AI/ML (SageMaker) Nền tảng Toàn diện: Bao phủ toàn bộ vòng đời từ chuẩn bị dữ liệu đến triển khai. Chuẩn bị dữ liệu: Các chiến lược gán nhãn và làm sạch dữ liệu. Huấn luyện \u0026amp; Tinh chỉnh: Tối ưu hóa mô hình về hiệu năng và chi phí. Triển khai: Đưa mô hình lên môi trường production (thực tế) một cách hiệu quả. MLOps Tích hợp: Nêu bật khả năng tự động hóa và chuẩn hóa các đường ống (pipeline) ML. SageMaker Studio: Demo trực tiếp giao diện thống nhất để xây dựng, huấn luyện và triển khai mô hình. Generative AI với Amazon Bedrock Mô hình Nền tảng (FMs): Hướng dẫn so sánh và lựa chọn các mô hình hàng đầu: Claude: Khả năng suy luận cao. Llama: Mã nguồn mở và hiệu quả. Titan: Tích hợp nguyên bản (native) với AWS. Prompt Engineering (Kỹ thuật Prompt): Chain-of-Thought (Chuỗi suy luận): Chia nhỏ các tác vụ suy luận phức tạp. Few-shot learning (Học từ vài mẫu): Sử dụng các ví dụ mẫu để cải thiện kết quả đầu ra. Kiến trúc Nâng cao: RAG (Retrieval-Augmented Generation): Tích hợp Knowledge Bases (Cơ sở tri thức) để căn cứ câu trả lời dựa trên dữ liệu doanh nghiệp. Bedrock Agents: Tạo quy trình làm việc đa bước và tích hợp với các công cụ bên ngoài. Guardrails (Bộ lọc an toàn): Đảm bảo an toàn và lọc nội dung. Bài học Chính (Key Takeaways) Tư duy Thiết kế Lựa chọn Mô hình: Chọn Mô hình nền tảng phù hợp (ví dụ: Claude vs. Titan) dựa trên yêu cầu cụ thể của trường hợp sử dụng (tốc độ vs. khả năng suy luận). An toàn là trên hết: Việc triển khai Guardrails là rất quan trọng đối với ứng dụng AI có trách nhiệm trong môi trường doanh nghiệp. Kiến trúc Kỹ thuật Ưu tiên RAG hơn Fine-tuning: Đối với hầu hết các trường hợp sử dụng kiến thức nội bộ, RAG mang lại giải pháp linh hoạt và tiết kiệm chi phí hơn so với việc tinh chỉnh mô hình. Quy trình làm việc Agentic: Chuyển từ chatbot thụ động sang các Agent chủ động có thể thực thi tác vụ là biên giới tiếp theo của GenAI. Chiến lược Hiện đại hóa Áp dụng MLOps: Chuyển từ huấn luyện mô hình thủ công sang các pipeline tự động (MLOps) là điều cần thiết để mở rộng quy mô. Bối cảnh Địa phương: Hiểu rõ các xu hướng AI/ML cụ thể tại Việt Nam giúp đối chiếu tiêu chuẩn cho các dự án trong nước. Ứng dụng vào Công việc Thử nghiệm SageMaker: Đánh giá quy trình ML hiện tại và xác định cơ hội chuyển đổi sang Amazon SageMaker để quản lý vòng đời tốt hơn. Xây dựng Bot Kiến thức: Tạo nguyên mẫu sử dụng Amazon Bedrock và RAG để tra cứu tài liệu nội bộ hoặc hướng dẫn kỹ thuật. Cải thiện Prompt: Áp dụng ngay kỹ thuật Chain-of-Thought và Few-shot để nâng cao độ chính xác của các tương tác AI hiện tại. Triển khai Guardrails: Cấu hình bộ lọc nội dung trên Bedrock để đảm bảo an toàn thương hiệu cho các ứng dụng thử nghiệm. Trải nghiệm Sự kiện Tham dự hội thảo “AWS AI/ML Services \u0026amp; Generative AI” đã cung cấp một lộ trình thực tế để áp dụng các dịch vụ thông minh. Những trải nghiệm chính bao gồm:\nHọc hỏi từ chuyên gia Hiểu rõ hơn về bức tranh AI/ML tại Việt Nam, nắm bắt các cơ hội và thách thức tại địa phương. Đào sâu kiến thức kỹ thuật về sự khác biệt giữa các Mô hình nền tảng lớn (Claude, Llama, Titan). Tiếp cận kỹ thuật thực tế Phần hướng dẫn SageMaker Studio đã minh họa cách thống nhất chuỗi công cụ ML rời rạc vào một giao diện quản lý duy nhất. Demo trực tiếp về Bedrock đã cho thấy việc triển khai thực tế một chatbot GenAI, giải mã sự phức tạp của RAG và Agents. Kết nối và thảo luận Hoạt động ice-breaker và các phiên networking cho phép trao đổi ý tưởng với đồng nghiệp về những thách thức thực tế khi triển khai GenAI. Các cuộc thảo luận củng cố tầm quan trọng của Prompt Engineering như một kỹ năng quan trọng cho phát triển hiện đại. Bài học rút ra RAG là chìa khóa để làm cho LLM hữu ích với dữ liệu đặc thù của doanh nghiệp mà không tốn chi phí huấn luyện cao. Guardrails không phải là tùy chọn; chúng là một lớp nền tảng của ngăn xếp (stack) Generative AI. Hiệu quả trong ML đến từ MLOps tích hợp thay vì các thử nghiệm khoa học dữ liệu riêng lẻ. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/1.1-week1/","title":"Worklog Tuần 1","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 1: Kết nối, làm quen với các thành viên trong First Cloud Journey. Hiểu dịch vụ AWS cơ bản Thực hành tạo tài khoản AWS free tier Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Kết nối với các thành viên FCJ - Đọc và lưu ý các nội quy, quy định tại đơn vị thực tập 08/09/2025 08/09/2025 3 - Tìm hiểu về tài khoản AWS - Thực hành::- Tạo tài khoản AWS Free Tier 09/09/2025 09/09/2025 4 - Giới thiệu về điện toán đám mây 10/09/2025 10/09/2025 5 - Tìm hiểu các công cụ quản lý tài khoản và chi phí của AWS, - Tìm hiểu về AWS support 11/09/2025 11/09/2025 6 - Thực hành: Tạo ngân sách cho tài khoản và admin group và admin user 12/09/2025 12/09/2025 Kết quả đạt được tuần 1: Đã tạo và cấu hình AWS Free Tier account thành công.\nLàm quen với AWS Management Console và biết cách tìm, truy cập, sử dụng dịch vụ từ giao diện web.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/1.2-week2/","title":"Worklog Tuần 2","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 2: Hiểu sâu về kiến trúc mạng trong AWS: VPC, Subnet, Route Table. Nắm vững các cơ chế bảo mật mạng: Security Group và NACL. Thực hành: Tự xây dựng một hệ thống mạng (Custom VPC) hoàn chỉnh thay vì dùng Default VPC. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu lý thuyết về Amazon VPC, Region, Availability Zones (AZ) - Hiểu về CIDR Blocks và cách chia dải IP 15/09/2025 15/09/2025 3 - Tìm hiểu các thành phần: Subnet (Public/Private), Route Tables, Internet Gateway (IGW) - Tìm hiểu về NAT Gateway (để Private subnet ra internet) 16/09/2025 16/09/2025 4 - Thực hành: Tạo Custom VPC, chia 2 Subnet (1 Public, 1 Private), gắn Internet Gateway 17/09/2025 17/09/2025 5 - Phân biệt Security Group (Stateful) và Network ACL (Stateless) - Cấu hình Inbound/Outbound rules cơ bản (SSH, HTTP) 18/09/2025 18/09/2025 6 - Thực hành: Hoàn thiện mô hình VPC 2 lớp. Kiểm tra kết nối mạng giữa các subnet và ra internet. 19/09/2025 19/09/2025 Kết quả đạt được tuần 2: Đã hiểu rõ luồng đi của gói tin trong VPC (Traffic flow).\nTự tay dựng được một kiến trúc VPC an toàn bao gồm: Public Subnet (cho Web Server) và Private Subnet (cho Database sau này).\nBiết cách cấu hình Security Group để chỉ mở các port cần thiết (Least Privilege).\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/5-workshop/5.2-prerequiste/","title":"Các bước chuẩn bị","tags":[],"description":"","content":"IAM permissions Gắn IAM permission policy sau vào tài khoản aws user của bạn để triển khai và dọn dẹp tài nguyên trong workshop này. Do tôi đang thực hiện workshop này lần đầu nên tôi sẽ gắn full quyền cho các quyền tôi đã chọn\n{ { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;polly:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;logs:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } } "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/5-workshop/5.4-lambda/create-lambda/","title":"Tạo Lambda","tags":[],"description":"","content":"Tạo hàm Lambda Truy cập vào Lambda management console Tại giao diện Dashboard, chọn Create function Trong giao diện Create Lambda function Name the lambda: chọn một tên chưa trùng lặp (gợi ý: số bài lab và tên của bạn) Runtime: chọn python 3.13 hoặc mới nhất Mở rộng phần Change default execution role Tại mục Execution role, chọn Use an existing role và chọn PollyLambdaRole Kéo xuống dưới và nhấn Create function (hoặc Create lambda) Tạo thành công Lambda function. Tạo code Lambda Trong bài thực hành này, chúng ta sẽ sử dụng code để chuyển đổi một thư mục văn bản thành thư mục giọng nói (Text-to-Speech) bằng dịch vụ Amazon Polly. Kéo xuống phần Code source. Xóa tất cả code hiện có trong file lambda_function.py. Chúng ta sẽ sử dụng đoạn code như sau: import json import boto3 import os s3 = boto3.client(\u0026#39;s3\u0026#39;) polly = boto3.client(\u0026#39;polly\u0026#39;) def lambda_handler(event, context): try: # 1. Lấy thông tin file vừa upload record = event[\u0026#39;Records\u0026#39;][0] bucket_name = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] object_key = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] # VD: input/hello.txt print(f\u0026#34;Processing file: {object_key}\u0026#34;) # 2. Đọc nội dung file text file_obj = s3.get_object(Bucket=bucket_name, Key=object_key) text_content = file_obj[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) # 3. Gọi Polly để chuyển đổi văn bản sang giọng nói (Giọng: Joanna) response = polly.synthesize_speech( Text=text_content, OutputFormat=\u0026#39;mp3\u0026#39;, VoiceId=\u0026#39;Joanna\u0026#39; ) # 4. Lưu file MP3 sang thư mục output # Đổi tên từ input/abc.txt thành output/abc.mp3 new_key = object_key.replace(\u0026#34;input/\u0026#34;, \u0026#34;output/\u0026#34;).replace(\u0026#34;.txt\u0026#34;, \u0026#34;.mp3\u0026#34;) if \u0026#34;AudioStream\u0026#34; in response: with response[\u0026#34;AudioStream\u0026#34;] as stream: s3.put_object( Bucket=bucket_name, Key=new_key, Body=stream.read(), ContentType=\u0026#39;audio/mpeg\u0026#39; ) return \u0026#34;Done!\u0026#34; except Exception as e: print(e) raise e Nhấn nút Deploy. Tạo Trigger Ngay phía trên phần code, nhấn nút + Add trigger. Source: Chọn S3. Bucket: Chọn bucket s3-demo-text. Event types: Chọn All object create events. Prefix: Nhập input/ ⚠️ Lưu ý: Bạn bắt buộc phải nhập input/. Nếu để trống, Lambda sẽ kích hoạt ngay cả khi file MP3 được tạo ra -\u0026gt; Gây ra vòng lặp vô tận -\u0026gt; Tăng chi phí. Suffix: Nhập .txt Tích vào ô \u0026ldquo;I acknowledge\u0026hellip;\u0026rdquo; -\u0026gt; Nhấn Add. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/1.12-week12/","title":"Worklog Tuần 12","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 12: Hệ thống hóa lại kiến thức về Serverless (Lambda) và Containers (ECS). Củng cố kỹ năng về Infrastructure as Code (CDK) và CI/CD Pipeline. Thực Hành Workshop 1 - Automated Text-to-Speech Converter using Serverless Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Ôn tập Tuần 7 \u0026amp; 8: So sánh ưu nhược điểm khi chọn Serverless (Lambda) vs Containers (ECS). - Review lại cách đóng gói Docker Image tối ưu. 24/11/2025 24/11/2025 3 - Ôn tập Tuần 9 (IaC): Đọc lại code CDK cũ, đảm bảo hiểu cấu trúc Construct và Stack. - Thử viết một đoạn code CDK ngắn tạo tài nguyên lạ (VD: SNS Topic) để kiểm tra phản xạ. 25/11/2025 25/11/2025 4 - Ôn tập Tuần 10 (CI/CD): Vẽ lại sơ đồ luồng chạy của Pipeline (Source -\u0026gt; Build -\u0026gt; Deploy). - Review các lỗi thường gặp khi Build (thiếu quyền, sai file config). 26/11/2025 26/11/2025 5 - Ôn tập Tuần 11 (Monitoring): Kiểm tra xem Lab tích hợp hôm qua đã có Logs và Metrics chưa. 6 Thực Hành Workshop 1 Automated Text-to-Speech Converter using Serverless 03/12/2025 07/12/2025 Kết quả đạt được tuần 12: Đã xâu chuỗi được mối liên hệ giữa các công cụ DevOps (Code -\u0026gt; Build -\u0026gt; Deploy -\u0026gt; Monitor).\nTự tin hơn trong việc debug các vấn đề liên quan đến phân quyền (IAM Roles) trong Pipeline và Lambda.\nHoàn thành bài Lab tích hợp khó (IaC + CI/CD + Containers)\nHoàn thành workshop 1 và demo\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/1.11-week11/","title":"Worklog Tuần 11","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 11: Giám sát sức khỏe hệ thống và xử lý sự cố. Kiểm toán các hoạt động trên tài khoản AWS. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu Amazon CloudWatch: Metrics, Logs, Alarms - Các metric chuẩn của EC2 (CPU, Network\u0026hellip;) 17/11/2025 17/11/2025 3 - Thực hành: Tạo CloudWatch Alarm gửi email qua SNS khi CPU EC2 \u0026gt; 80% - Cài CloudWatch Agent để lấy thông số RAM (Memory usage) 18/11/2025 18/11/2025 4 - Tìm hiểu AWS CloudTrail (Audit log) - Tìm hiểu AWS X-Ray (Tracing cho microservices) 19/11/2025 19/11/2025 5 - Thực hành: Tạo Dashboard trên CloudWatch để quan sát tổng quan hệ thống - Kiểm tra CloudTrail để xem ai đã xóa resource 20/11/2025 20/11/2025 6 - Review CouldTrail và CouldWatch 15/08/2025 15/08/2025 Kết quả đạt được tuần 11: Có khả năng giám sát hệ thống thời gian thực.\nThiết lập được hệ thống cảnh báo sớm sự cố.\nBiết cách truy vết (audit) các hành động nhạy cảm trên tài khoản AWS.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/1-worklog/1.10-week10/","title":"Worklog Tuần 10","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nMục tiêu tuần 10: Tự động hóa quy trình phát triển phần mềm (DevOps). Xây dựng pipeline CI/CD hoàn chỉnh trên AWS. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu về CI/CD (Continuous Integration / Continuous Delivery) - Giới thiệu bộ công cụ: CodeCommit, CodeBuild, CodeDeploy, CodePipeline 10/11/2025 10/11/2025 3 - Thực hành: Tạo Repository (có thể dùng GitHub), kết nối với CodeBuild - Viết file buildspec.yml để build code 11/11/2025 11/11/2025 4 - Tìm hiểu CodePipeline: Source stage, Build stage, Deploy stage - Thực hành: Tạo Pipeline đơn giản deploy file lên S3 12/11/2025 12/11/2025 5 - Thực hành nâng cao: Tạo Pipeline deploy ứng dụng lên ECS hoặc Lambda - Thêm bước \u0026ldquo;Manual Approval\u0026rdquo; trước khi deploy production 13/11/2025 13/11/2025 6 - Test luồng: Push code -\u0026gt; Tự động Build -\u0026gt; Tự động Deploy. 14/11/2025 14/11/2025 Kết quả đạt được tuần 10: Hiểu quy trình DevOps tiêu chuẩn.\nĐã xây dựng được pipeline tự động hóa: chỉ cần commit code, hệ thống tự động build và deploy phiên bản mới.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/2-proposal/","title":"Bản đề xuất","tags":[],"description":"","content":"Web bán hàng MealPlan Bấm để tải về báo cáo (.docx)\n1. Tóm tắt điều hành Nền tảng Bán Nguyên liệu Thực phẩm Cá nhân hóa tập trung vào việc cho phép mua sắm nhanh hơn và hiệu quả hơn. Người dùng đăng ký tài khoản để truy cập cơ sở dữ liệu công thức đa dạng, nhận đề xuất bữa ăn dựa trên AI dựa trên lịch sử mua hàng và đặt hàng giao tận nhà. Tận dụng cơ sở hạ tầng đám mây AWS, nền tảng đảm bảo khả năng mở rộng linh hoạt, hiệu suất cao và quản lý an toàn.\n2. Tuyên bố vấn đề Vấn đề hiện tại\nKhách hàng thường mất nhiều thời gian tìm kiếm những bữa ăn phù hợp với nhu cầu hàng ngày. Mặc dù nhiều nền tảng cung cấp gợi ý về bữa ăn hoặc thực đơn, nhưng hầu hết đều không hỗ trợ mua các món ăn hoàn chỉnh, chế biến sẵn, buộc người dùng phải tự tìm kiếm nhà hàng hoặc nhà cung cấp những bữa ăn đó.\nGiải pháp\nNền tảng sử dụng Spring Boot để xây dựng một backend ổn định với các API REST cho tài khoản người dùng, công thức nấu ăn, giỏ hàng và đơn hàng. Frontend được xây dựng bằng React và cung cấp các đề xuất bữa ăn dựa trên AI dễ sử dụng. Dữ liệu được lưu trữ trong AWS RDS (PostgreSQL), trong khi hình ảnh và tệp tĩnh được lưu trữ trên Amazon S3. Backend chạy trên Amazon EC2 bên trong một VPC an toàn và Route 53 được sử dụng để quản lý tên miền.\nLợi ích và hoàn vốn đầu tư\nGiải pháp này thiết lập một nền tảng toàn diện cho công ty khởi nghiệp về dinh dưỡng để mở rộng dịch vụ, đồng thời thu thập dữ liệu người dùng cho các hệ thống khuyến nghị nâng cao. Chi phí là 119,51 đô la Mỹ/tháng và 1.434,12 đô la Mỹ/12 tháng. Quá trình phát triển tận dụng các khuôn khổ mã nguồn mở, không phát sinh thêm chi phí phần cứng.\n3. Kiến trúc giải pháp Trang web được host trên EC2. Dữ liệu được lưu trữ bằng EC2 instance. Hình ảnh được lưu trên S3. Code sẽ dược đẩy lên github nhằm quản lý và tự động đẩy code lên s3 để CodeDeploy sẽ thực hiện deploy lên server. Cloudfront được sử dụng nhằm cải thiện tốc tải. Cognito dùng để quản lý danh tính người dùng. CloudTrail được dùng để giám sát và lữu trữ lịch sử hoạt động. CloudWatch dùng để giám sát và quản lý hiệu suất, tình trạng hoạt động của các tài nguyên và ứng dụng trên AWS. IAM dùng để cấp quyền cho các service. SecretManager được dùng nhằm quản lý các thông tin nhạy cảm.\nDịch vụ AWS sử dụng\nWAF: Bảo vệ ứng dụng web khỏi các tấn công mạng AWS CloudFront: Tăng tốc độ tải trang web. AWS EC2: Deploy sản phẩm, NAT instance, Database. AWS VPC: là mạng ảo. AWS S3: Lưu trữ code, file log, hình ảnh. CodeDeploy: Deploy code lên EC2. GitLab: chứa source code và push code lên s3. Amazon Cognito: Quản lý quyền truy cập cho người dùng trang web. IAM: Tạo user và role. Secret Manager: Chứa các thông tin quan trọng. CloudTrail: Giám sát và lữu trữ lịch sử hoạt động. CloudWatch: Giám sát và quản lý hiệu suất, tình trạng hoạt động của các tài nguyên và ứng dụng trên AWS 4. Triển khai kỹ thuật Các giai đoạn triển khai\nCác giai đoạn triển khai Dự án này gồm hai phần: phát triển back-end Spring Boot và front-end React, triển khai website lên AWS bằng các dịch vụ AWS, mỗi phần gồm 4 giai đoạn.\nXây dựng Lý thuyết và Vẽ Kiến trúc: Thu thập các yêu cầu cho ứng dụng web, thiết kế kiến ​​trúc hệ thống (Spring Boot REST API + React front-end) và định nghĩa lược đồ cơ sở dữ liệu (Tháng 1).\nPhát triển, Kiểm thử: Triển khai back-end Spring Boot với các REST API (xác thực, quản lý người dùng, CRUD món ăn/công thức, giỏ hàng, v.v.) và xây dựng front-end React (UI/UX, định tuyến, biểu mẫu, quản lý trạng thái). Thực hiện các bài kiểm tra đơn vị cho các dịch vụ back-end, kiểm tra tích hợp cho các điểm cuối API và kiểm tra front-end (Thư viện Kiểm tra Jest/React). (Tháng 1–2)\nTính toán Giá và Kiểm tra Tính khả thi: Sử dụng Công cụ Tính giá AWS để ước tính chi phí cho EC2 (lưu trữ back-end), RDS (cơ sở dữ liệu), S3 (tệp và hình ảnh tĩnh), VPC (mạng), Route53 (tên miền) sau đó điều chỉnh nếu cần (Tháng 2).\nTích hợp AWS: Tích hợp các dịch vụ AWS vào ứng dụng. Triển khai trang web trên EC2, lưu trữ hình ảnh trên S3, cấu hình RDS cho cơ sở dữ liệu, sử dụng VPC để quản lý mạng, Route53 cho miền và thiết lập các pipeline CI/CD (GitHub Actions hoặc AWS CodePipeline). Thực hiện kiểm thử dàn dựng trước khi phát hành chính thức. (Tháng 3)\nYêu cầu kỹ thuật\nBack-end (Spring Boot): REST API để xác thực, quản lý người dùng, CRUD món ăn/công thức nấu ăn, giỏ hàng và xử lý đơn hàng. Bao gồm bảo mật (JWT, Spring Security). Front-end (React): Ứng dụng web đáp ứng với UI/UX thân thiện với người dùng, tích hợp API với back-end Spring Boot. Cơ sở dữ liệu (EC2): Cơ sở dữ liệu quan hệ (MySQL/PostgreSQL) để lưu trữ người dùng, công thức nấu ăn, giỏ hàng và dữ liệu đơn hàng được host trên EC2. Lưu trữ (AWS S3): Được sử dụng để lưu trữ hình ảnh do người dùng tải lên Lưu trữ \u0026amp; Mạng (AWS EC2 \u0026amp; AWS VPC): Ứng dụng được triển khai trên các phiên bản EC2. 6, CI/CD (GitHub Actions hoặc AWS CodePipeline): Quy trình xây dựng, triển khai tự động cho cả back-end và front-end. Xác thực \u0026amp; Bảo mật: Xác thực JWT và cấu hình HTTPS; AWS Cognito tùy chọn để quản lý quyền truy cập của người dùng. 5. Lộ trình \u0026amp; Mốc triển khai Tháng 1: Xây dựng lý thuyết và vẽ kiến ​​trúc (backend Spring Boot + thiết kế frontend React, lược đồ cơ sở dữ liệu). Bắt đầu phát triển ban đầu backend và frontend.\nTháng 2: Tiếp tục phát triển backend và frontend, thực hiện kiểm thử đơn vị và tích hợp. Sử dụng Công cụ tính giá AWS để đánh giá chi phí lưu trữ và tinh chỉnh kiến ​​trúc nhằm đảm bảo hiệu quả chi phí.\nTháng 3: Tích hợp các dịch vụ AWS, cấu hình các pipeline CI/CD, tiến hành kiểm thử dàn dựng và triển khai website lên môi trường production.\nSau khi ra mắt: Tối đa 3 tháng để bảo trì, tối ưu hóa và cải tiến tính năng\n6. Ước tính ngân sách Có thể xem chi phí trên AWS Pricing Calculator\nDịch Vụ AWS (AWS Services) AWS WAF: $11.6/tháng Application Load Balancer (ALB): $18.63/tháng Amazon EC2 Application: $19.27/tháng Amazon EC2 Data Tier: $9.64/tháng Amazon EC2 Nat Instances: $19.27/tháng Amazon S3: $3.72/tháng AWS CodeDeploy: 0$ AWS Secrets Manager: $0.4/tháng Amazon Cognito: $14.25/tháng Amazon CloudWatch: $4.91/tháng AWS CloudTrail: $1.77/tháng VPC Endpoints: $16.05/tháng Total: $119.51/tháng, $1434.12/12 tháng 7. Đánh giá rủi ro Ma trận rủi ro\n⦁\tMất kết nối mạng: Ảnh hưởng trung bình / Xác suất trung bình. ⦁\tHỏng/Treo EC2 / ALB / AZ: Ảnh hưởng cao / Xác suất thấp–trung bình. ⦁\tRò rỉ bí mật: Ảnh hưởng cao / Xác suất thấp–trung bình. ⦁\tVượt chi phí: Ảnh hưởng trung bình / Xác suất trung bình.\nChiến lược giảm thiểu\n⦁\tTính khả dụng: Multi-AZ + Auto Scaling + ALB; health checks; caching bằng CloudFront; Route 53 failover. Sử dụng caching của CloudFront để giảm sự phụ thuộc vào dịch vụ backend. ⦁\tBảo mật: IAM theo nguyên tắc ít quyền nhất (least-privilege); Secrets Manager với cơ chế xoay vòng (rotation) và ghi nhật ký truy cập; bảo vệ bằng WAF/Shield. ⦁\tQuản lý chi phí: AWS Budgets \u0026amp; Cost Explorer; điều chỉnh kích thước phù hợp (rightsizing); sử dụng Reserved hoặc Spot instances khi phù hợp.\nKế hoạch dự phòng\n⦁\tTự động rollback thông qua CodeDeploy. ⦁\tDự phòng thủ công: GitLab runner → prebuilt AMIs hoặc ASGs\n8. Kết quả kỳ vọng ⦁\tCI/CD hoàn toàn tự động (GitLab → CodePipeline → CodeBuild → CodeDeploy) giảm lỗi thủ công. ⦁\tBảo mật nhiều lớp (IAM, WAF, Secrets Manager) với audit qua CloudTrail.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/3-blogstranslated/","title":"Các bài blogs đã dịch","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTại đây sẽ là phần liệt kê, giới thiệu các blogs mà các bạn đã dịch. Ví dụ:\nBlog 1 - Getting started with healthcare data lakes: Using microservices Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 2 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 3 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 4 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 5 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\nBlog 6 - \u0026hellip; Blog này giới thiệu cách bắt đầu xây dựng data lake trong lĩnh vực y tế bằng cách áp dụng kiến trúc microservices. Bạn sẽ tìm hiểu vì sao data lake quan trọng trong việc lưu trữ và phân tích dữ liệu y tế đa dạng (hồ sơ bệnh án điện tử, dữ liệu xét nghiệm, thiết bị IoT y tế…), cách microservices giúp hệ thống linh hoạt, dễ mở rộng và dễ bảo trì hơn. Bài viết cũng hướng dẫn các bước khởi tạo môi trường, tổ chức pipeline xử lý dữ liệu, và đảm bảo tuân thủ các tiêu chuẩn bảo mật \u0026amp; quyền riêng tư như HIPAA.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/4-eventparticipated/","title":"Các events đã tham gia","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTrong phần này, các bạn cần liệt kê và mô tả chi tiết các sự kiện (event) mà mình đã tham gia trong suốt quá trình thực tập hoặc làm việc.\nMỗi sự kiện nên được trình bày theo định dạng Event 1, Event 2, Event 3…, kèm theo các thông tin:\nTên sự kiện Thời gian tổ chức Địa điểm (nếu có) Vai trò của bạn trong sự kiện (người tham dự, hỗ trợ tổ chức, diễn giả, v.v.) Mô tả ngắn gọn nội dung và hoạt động chính trong sự kiện Kết quả hoặc giá trị đạt được (bài học, kỹ năng mới, đóng góp cho nhóm/dự án) Việc liệt kê này giúp thể hiện rõ sự tham gia thực tế của bạn, cũng như các kỹ năng mềm và kinh nghiệm bạn đã tích lũy qua từng sự kiện. Trong quá trình thực tập, em đã tham gia 4 events, với mỗi event là một trải nghiệm đáng nhớ với những kiến thức mới, hay và bổ ích, cùng với đó là nhứng món quà và những khoảnh khắc rất tuyệt vời.\nEvent 1 Tên sự kiện: Cloud Day\nThời gian: 09:00 ngày 18/09/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 2 Tên sự kiện: Event AWS Cloud Mastery Series #1\nThời gian: 09:00 ngày 15/11/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 3 Tên sự kiện: Event AWS Cloud Mastery Series #2\nThời gian: 09:00 ngày 17/11/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 4 Tên sự kiện: Event AWS Cloud Mastery Series #3\nThời gian: 09:00 ngày 29/11/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/5-workshop/5.5-testing/","title":"Kiểm tra","tags":[],"description":"","content":"Tạo file đầu vào Trên máy tính của bạn, tạo một file có tên là test.txt. Tạo file với nội dung: \u0026ldquo;Hello, congratulations on completing the workshop!\u0026rdquo; Lưu file lại. Quay lại tab S3, mở thư mục input. Nhấn Upload -\u0026gt; Chọn file test.txt -\u0026gt; Nhấn Upload Chờ khoảng 5 giây.\nQuay lại giao diện Bucket, sau đó mở thư mục output. Bạn sẽ thấy file test.mp3. Chọn nó và nhấn Download.\nMở file lên để nghe thử.\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nHệ thống chuyển đổi Văn bản sang Giọng nói tự động sử dụng Serverless Tổng quan Kiến trúc hướng sự kiện (Event-Driven Architecture) cho phép bạn xây dựng các ứng dụng tự động phản hồi lại các thay đổi trạng thái, chẳng hạn như khi tải lên một tập tin, mà không cần cung cấp hay quản lý máy chủ.\nTrong bài thực hành này, bạn sẽ học cách tạo, cấu hình và kiểm thử một quy trình Serverless có khả năng tự động chuyển đổi các file văn bản thành âm thanh giọng nói sống động bằng cách sử dụng các Dịch vụ được quản lý của AWS (AWS Managed Services).\nBạn sẽ tận dụng hai cơ chế chính để xử lý dữ liệu bất đồng bộ:\nS3 Event Notifications - Cấu hình Amazon S3 đóng vai trò là nguồn sự kiện. Nó sẽ tự động kích hoạt một hàm tính toán bất cứ khi nào có một đối tượng mới được tải lên một thư mục đầu vào cụ thể. Managed AI Services - Tận dụng Amazon Polly để tổng hợp giọng nói từ văn bản. Điều này cho phép bạn thêm các khả năng học sâu (deep learning) vào ứng dụng của mình thông qua các lệnh gọi API đơn giản mà không cần chuyên môn về khoa học dữ liệu. Nội dung Tổng quan Workshop Các bước chuẩn bị Xây dựng Logic xử lý (Lambda) Cấu hình Lưu trữ (S3) Kiểm tra kết quả Dọn dẹp tài nguyên "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/5-workshop/5.6-cleanup/","title":"Dọn dẹp tài nguyên","tags":[],"description":"","content":"Chúc mừng bạn đã hoàn thành workshop này! Trong workshop này, bạn đã tìm hiểu các mô hình kiến trúc để xây dựng ứng dụng Serverless hướng sự kiện (Event-driven) trên AWS.\nBằng cách cấu hình S3 Event Notifications, bạn đã kích hoạt một quy trình tự động hóa nơi tài nguyên tính toán (AWS Lambda) phản ứng tức thì với việc nạp dữ liệu mà không cần can thiệp thủ công hay quản lý máy chủ.\nBằng cách tích hợp Amazon Polly, bạn đã tận dụng thành công các Dịch vụ AI được quản lý (Managed AI Services) để chuyển đổi văn bản thành giọng nói sống động, chứng minh cách thêm các khả năng máy học phức tạp vào ứng dụng của mình với lượng code tối thiểu.\nDọn dẹp tài nguyên Xóa S3 buckets Mở giao diện S3 console Mở bucket s3-demo-bucket Chọn 2 thư mục input và output Nhấn Delete và xác nhận Sau đó chọn bucket mà chúng ta đã tạo cho bài lab, nhấn Empty (làm rỗng) và xác nhận. Nhấn Delete (xóa) và xác nhận xóa. Xóa Lambda function Mở giao diện Lambda console Tìm function workshop1 và nhấn Actions Chọn Delete và xác nhận Xóa IAM role Mở giao diện IAM console Chọn Roles từ menu bên trái. Tìm role PollyLambdaRole. Chọn role đó và nhấn Delete. "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/6-self-evaluation/","title":"Tự đánh giá","tags":[],"description":"","content":"Trong suốt thời gian thực tập tại First Cloud Journey program từ 08/09/2025 đến 09/12/2025, tôi đã có cơ hội học hỏi, rèn luyện và áp dụng kiến thức đã được trang bị tại trường vào môi trường làm việc thực tế.\n\u0026ldquo;Tôi đã có cơ hội tìm hiểu về công nghệ điện toán đám mây và vận dụng những kiến thức đã học vào nhiều dự án thực tế. Việc nghiên cứu các dịch vụ AWS và tham gia các buổi workshop đã giúp tôi hiểu sâu hơn về lĩnh vực này. Ngoài ra, đây cũng là dịp để tôi mở rộng mạng lưới quan hệ của mình.\u0026rdquo;.\nVề tác phong, tôi luôn cố gắng hoàn thành tốt nhiệm vụ, tuân thủ nội quy, và tích cực trao đổi với đồng nghiệp để nâng cao hiệu quả công việc.\nĐể phản ánh một cách khách quan quá trình thực tập, tôi xin tự đánh giá bản thân dựa trên các tiêu chí dưới đây:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Hiểu biết về ngành, áp dụng kiến thức vào thực tế, kỹ năng sử dụng công cụ, chất lượng công việc ☐ ✅ ☐ 2 Khả năng học hỏi Tiếp thu kiến thức mới, học hỏi nhanh ✅ ☐ ☐ 3 Chủ động Tự tìm hiểu, nhận nhiệm vụ mà không chờ chỉ dẫn ✅ ☐ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng ✅ ☐ ☐ 5 Kỷ luật Tuân thủ giờ giấc, nội quy, quy trình làm việc ✅ ☐ ☐ 6 Tính cầu tiến Sẵn sàng nhận feedback và cải thiện bản thân ✅ ☐ ☐ 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc rõ ràng ☐ ✅ ☐ 8 Hợp tác nhóm Làm việc hiệu quả với đồng nghiệp, tham gia nhóm ✅ ☐ ☐ 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác, môi trường làm việc ✅ ☐ ☐ 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp, sáng tạo ☐ ✅ ☐ 11 Đóng góp vào dự án/tổ chức Hiệu quả công việc, sáng kiến cải tiến, ghi nhận từ team ✅ ☐ ☐ 12 Tổng thể Đánh giá chung về toàn bộ quá trình thực tập ✅ ☐ ☐ Cần cải thiện Nâng cao các kỹ năng chuyên môn và kiến thức thực tế Cải thiện trong cách tư duy giải quyết vấn đề Học cách giao tiếp tốt hơn trong giao tiếp hằng ngày và trong công việc, xử lý tình huống "},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/7-feedback/","title":"Chia sẻ, đóng góp ý kiến","tags":[],"description":"","content":" ⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTại đây bạn có thể tự do đóng góp ý kiến cá nhân về những trải nghiệm khi tham gia chương trình First Cloud Journey, giúp team FCJ cải thiện những vấn đề còn thiếu sót dựa trên các hạng mục sau:\nĐánh giá chung 1. Môi trường làm việc\nMôi trường làm việc thân thiện, các mentor rất hòa đồng và hỗ trợ nhiệt tình. Họ luôn sẵn lòng giúp đỡ tôi khi tôi gặp bất kỳ vấn đề nào. Không gian làm việc gọn gàng và thoải mái, giúp tôi tập trung tốt hơn. Tuy nhiên, tôi nghĩ sẽ tốt hơn nếu thường xuyên tổ chức các buổi giao lưu hoặc hoạt động gắn kết nhóm để mọi nguời hiểu nhau hơn.\n2. Sự hỗ trợ của mentor / team admin\nMentor hướng dẫn rất chi tiết, giải thích rõ ràng khi mình chưa hiểu và luôn khuyến khích mình đặt câu hỏi. Team admin hỗ trợ các thủ tục, tài liệu và tạo điều kiện để mình làm việc thuận lợi. Mình đánh giá cao việc mentor cho phép mình thử và tự xử lý vấn đề thay vì chỉ đưa đáp án.\n3. Sự phù hợp giữa công việc và chuyên ngành học\nCông việc mình được giao phù hợp với kiến thức mình đã học ở trường, đồng thời mở rộng thêm những mảng mới mà mình chưa từng được tiếp cận. Nhờ vậy, mình vừa củng cố kiến thức nền tảng, vừa học thêm kỹ năng thực tế.\n4. Cơ hội học hỏi \u0026amp; phát triển kỹ năng\nTrong quá trình thực tập, mình học được nhiều kỹ năng mới như sử dụng công cụ quản lý dự án, kỹ năng làm việc nhóm, và cả cách giao tiếp chuyên nghiệp trong môi trường công ty. Mentor cũng chia sẻ nhiều kinh nghiệm thực tế giúp mình định hướng tốt hơn cho sự nghiệp.\n5. Văn hóa \u0026amp; tinh thần đồng đội\nVăn hóa công ty rất tích cực: mọi người tôn trọng lẫn nhau, làm việc nghiêm túc nhưng vẫn vui vẻ. Khi có dự án gấp, mọi người cùng nhau cố gắng, hỗ trợ không phân biệt vị trí. Điều này giúp mình cảm thấy mình là một phần của tập thể, dù chỉ là thực tập sinh.\n6. Chính sách / phúc lợi cho thực tập sinh\nCông ty thường xuyên tạo cơ hội cho tôi kết nối với các cố vấn khác để cải thiện mạng lưới quan hệ của mình. Thêm vào đó, tôi đã học được các kỹ năng giao tiếp – điều này giúp tôi tự tin hơn khi giao tiếp với đồng nghiệp, điều mà trước đây tôi không làm được.\nMột số câu hỏi khác Điều bạn hài lòng nhất trong thời gian thực tập? Điều tôi hài lòng nhất là môi trường hoạt động của doanh nghiệp và các anh chị mentor thân thiện trong thời gian tôi thực tập tại công ty Điều bạn nghĩ công ty cần cải thiện cho các thực tập sinh sau? Tôi nghĩ là công ty nên tổ chức thêm các buổi giao lưu với nhau để cùng tìm hiểu thêm về các công nghệ mới trong tương lai. Nếu giới thiệu cho bạn bè, bạn có khuyên họ thực tập ở đây không? Vì sao? Chắc chắn là có. Vì môi trường tích cực, linh động là điều mà phù hợp với các bạn mới bắt đầu làm quen với công việc và tiếp xúc nhiều công nghệ mới.\nĐề xuất \u0026amp; mong muốn Bạn có đề xuất gì để cải thiện trải nghiệm trong kỳ thực tập? Em tạm thời không có ý kiến\nBạn có muốn tiếp tục chương trình này trong tương lai? Nếu như có cơ hội được đồng hành cùng công ty, tôi nghĩ rằng tôi sẽ tiếp tục và tham gia các chứng chỉ của AWS\n"},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://doquockhanh2005.github.io/DoQuocKhanh_FPTU_FCJ/vi/tags/","title":"Tags","tags":[],"description":"","content":""}]